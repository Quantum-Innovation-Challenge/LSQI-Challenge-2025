{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Simulation done on Google Collab"
      ],
      "metadata": {
        "id": "pZb1XYQaSJmS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMzGnB9ASC4r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"/content/drive/MyDrive/QIC2025-EstDat.csv\")"
      ],
      "metadata": {
        "id": "iXfxGrl8SPeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.optimize import curve_fit\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "-BdTTgH8SU1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EDA for Selection of Apt PK PD Design"
      ],
      "metadata": {
        "id": "Ao359BBdSi3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10"
      ],
      "metadata": {
        "id": "wICm3VSNSZHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "print(\"\\nDataset Info:\")\n",
        "print(df.info())\n",
        "print(\"\\nSummary Statistics:\")\n",
        "print(df.describe())"
      ],
      "metadata": {
        "id": "Z-YxN_CkSg65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_dataset_structure(df):\n",
        "    # Basic data characterization\n",
        "    print(\"\\nData Distribution:\")\n",
        "    print(f\"Unique subjects: {df['ID'].nunique()}\")\n",
        "    print(f\"Dose levels: {sorted(df[df['DOSE'] > 0]['DOSE'].unique())}\")\n",
        "    print(f\"Time range: {df['TIME'].min():.1f} - {df['TIME'].max():.1f} hours\")\n",
        "    print(f\"Body weight range: {df['BW'].min():.1f} - {df['BW'].max():.1f} kg\")\n",
        "    print(f\"COMED distribution: {df['COMED'].value_counts().to_dict()}\")\n",
        "\n",
        "    # Compartment analysis\n",
        "    print(f\"\\nCompartments (CMT): {sorted(df['CMT'].unique())}\")\n",
        "    print(f\"DVID types: {sorted(df['DVID'].unique())}\")\n",
        "\n",
        "    # Event analysis\n",
        "    dosing_events = df[df['EVID'] == 1]\n",
        "    obs_events = df[df['EVID'] == 0]\n",
        "    print(f\"\\nDosing events: {len(dosing_events)}\")\n",
        "    print(f\"Observation events: {len(obs_events)}\")\n",
        "    print(f\"Missing DV: {(df['MDV'] == 1).sum()}\")"
      ],
      "metadata": {
        "id": "dVr8RKJHSyOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compartment_structure_analysis(df):\n",
        "\n",
        "    print(\"\\n\\COMPARTMENT STRUCTURE ANALYSIS\")\n",
        "\n",
        "    # Separate PK and PD observations\n",
        "    pk_data = df[(df['DVID'] == 1) & (df['EVID'] == 0) & (df['MDV'] == 0)].copy()\n",
        "    pd_data = df[(df['DVID'] == 2) & (df['EVID'] == 0) & (df['MDV'] == 0)].copy()\n",
        "\n",
        "    print(f\"PK observations: {len(pk_data)}\")\n",
        "    print(f\"PD observations: {len(pd_data)}\")\n",
        "\n",
        "    # Plot concentration-time profiles by dose\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "    # PK concentration profiles by dose\n",
        "    ax1 = axes[0, 0]\n",
        "    dose_levels = sorted(pk_data[pk_data['DOSE'] > 0]['DOSE'].unique())\n",
        "    colors = plt.cm.Set1(np.linspace(0, 1, len(dose_levels)))\n",
        "\n",
        "    for i, dose in enumerate(dose_levels):\n",
        "        dose_data = pk_data[pk_data['DOSE'] == dose]\n",
        "\n",
        "        # Individual profiles (light lines)\n",
        "        for subject in dose_data['ID'].unique():\n",
        "            subj_data = dose_data[dose_data['ID'] == subject].sort_values('TIME')\n",
        "            ax1.plot(subj_data['TIME'], subj_data['DV'],\n",
        "                    color=colors[i], alpha=0.3, linewidth=0.5)\n",
        "\n",
        "        # Mean profile (bold line)\n",
        "        mean_profile = dose_data.groupby('TIME')['DV'].agg(['mean', 'std']).reset_index()\n",
        "        ax1.plot(mean_profile['TIME'], mean_profile['mean'],\n",
        "                color=colors[i], linewidth=2, label=f'{dose} mg', marker='o', markersize=4)\n",
        "        ax1.fill_between(mean_profile['TIME'],\n",
        "                        mean_profile['mean'] - mean_profile['std'],\n",
        "                        mean_profile['mean'] + mean_profile['std'],\n",
        "                        color=colors[i], alpha=0.2)\n",
        "\n",
        "    ax1.set_xlabel('Time (hours)')\n",
        "    ax1.set_ylabel('Concentration (mg/L)')\n",
        "    ax1.set_title('PK Profiles: Concentration vs Time')\n",
        "    ax1.legend()\n",
        "    ax1.set_yscale('log')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Semilog plot to check for biphasic decay\n",
        "    ax2 = axes[0, 1]\n",
        "    for i, dose in enumerate(dose_levels):\n",
        "        dose_data = pk_data[pk_data['DOSE'] == dose]\n",
        "        mean_profile = dose_data.groupby('TIME')['DV'].mean().reset_index()\n",
        "        # Focus on elimination phase (after 2 hours)\n",
        "        elim_data = mean_profile[mean_profile['TIME'] > 2]\n",
        "        if len(elim_data) > 5:\n",
        "            ax2.semilogy(elim_data['TIME'], elim_data['DV'],\n",
        "                        'o-', color=colors[i], label=f'{dose} mg', linewidth=2)\n",
        "\n",
        "    ax2.set_xlabel('Time (hours)')\n",
        "    ax2.set_ylabel('Concentration (mg/L) - Log Scale')\n",
        "    ax2.set_title('Elimination Phase Analysis')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # Check compartment-specific data\n",
        "    ax3 = axes[1, 0]\n",
        "    compartments = sorted(df['CMT'].unique())\n",
        "    for cmt in compartments:\n",
        "        cmt_data = df[(df['CMT'] == cmt) & (df['EVID'] == 0) & (df['MDV'] == 0)]\n",
        "        if len(cmt_data) > 0:\n",
        "            mean_by_time = cmt_data.groupby('TIME')['DV'].mean()\n",
        "            ax3.plot(mean_by_time.index, mean_by_time.values,\n",
        "                    'o-', label=f'CMT {cmt}', linewidth=2, markersize=4)\n",
        "\n",
        "    ax3.set_xlabel('Time (hours)')\n",
        "    ax3.set_ylabel('Mean DV')\n",
        "    ax3.set_title('Mean Response by Compartment')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    # Absorption analysis (early times)\n",
        "    ax4 = axes[1, 1]\n",
        "    early_pk = pk_data[pk_data['TIME'] <= 4]  # First 4 hours\n",
        "    for i, dose in enumerate(dose_levels):\n",
        "        dose_data = early_pk[early_pk['DOSE'] == dose]\n",
        "        if len(dose_data) > 0:\n",
        "            mean_profile = dose_data.groupby('TIME')['DV'].mean().reset_index()\n",
        "            ax4.plot(mean_profile['TIME'], mean_profile['DV'],\n",
        "                    'o-', color=colors[i], label=f'{dose} mg', linewidth=2)\n",
        "\n",
        "    ax4.set_xlabel('Time (hours)')\n",
        "    ax4.set_ylabel('Concentration (mg/L)')\n",
        "    ax4.set_title('Absorption Phase (0-4 hours)')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Analysis interpretation\n",
        "    print(\"\\nCompartment Structure Insights:\")\n",
        "\n",
        "    # Check for biphasic elimination\n",
        "    if len(dose_levels) > 0:\n",
        "        # Analyze longest dose group\n",
        "        max_dose_data = pk_data[pk_data['DOSE'] == max(dose_levels)]\n",
        "        elim_phase = max_dose_data[max_dose_data['TIME'] > 2]\n",
        "\n",
        "        if len(elim_phase) > 10:\n",
        "            # Simple check for curvature in log-concentration\n",
        "            time_elim = elim_phase.groupby('TIME')['DV'].mean()\n",
        "            log_conc = np.log(time_elim.values)\n",
        "            time_vals = time_elim.index.values\n",
        "\n",
        "            # Linear regression on log(conc) vs time\n",
        "            if len(time_vals) > 5:\n",
        "                slope, intercept, r_value, _, _ = stats.linregress(time_vals, log_conc)\n",
        "                print(f\"   • Elimination R² = {r_value**2:.3f}\")\n",
        "                if r_value**2 < 0.95:\n",
        "                    print(\"Poor linear fit in elimination → Consider 2-compartment model\")\n",
        "                else:\n",
        "                    print(\"Good linear fit → 1-compartment may be sufficient\")\n",
        "\n",
        "    return pk_data, pd_data"
      ],
      "metadata": {
        "id": "ZHLPEWIuTMYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def covariate_analysis(df, pk_data, pd_data):\n",
        "\n",
        "    print(\"\\n COVARIATE ANALYSIS\")\n",
        "\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "    # Body Weight Effects on PK\n",
        "    ax1 = axes[0, 0]\n",
        "    if len(pk_data) > 0:\n",
        "        # Calculate apparent clearance (Dose/AUC approximation)\n",
        "        pk_summary = []\n",
        "        for subject in pk_data['ID'].unique():\n",
        "            subj_data = pk_data[pk_data['ID'] == subject].sort_values('TIME')\n",
        "            if len(subj_data) > 3:\n",
        "                # Simple AUC approximation using trapezoidal rule\n",
        "                auc = np.trapz(subj_data['DV'], subj_data['TIME'])\n",
        "                dose = subj_data['DOSE'].iloc[0]\n",
        "                bw = subj_data['BW'].iloc[0]\n",
        "                comed = subj_data['COMED'].iloc[0]\n",
        "\n",
        "                if auc > 0:\n",
        "                    apparent_cl = dose / auc\n",
        "                    pk_summary.append({\n",
        "                        'ID': subject, 'BW': bw, 'COMED': comed,\n",
        "                        'DOSE': dose, 'AUC': auc, 'CL_app': apparent_cl\n",
        "                    })\n",
        "\n",
        "        pk_summary_df = pd.DataFrame(pk_summary)\n",
        "\n",
        "        if len(pk_summary_df) > 0:\n",
        "            # Plot CL vs BW\n",
        "            scatter = ax1.scatter(pk_summary_df['BW'], pk_summary_df['CL_app'],\n",
        "                                c=pk_summary_df['DOSE'], cmap='viridis', s=60, alpha=0.7)\n",
        "            plt.colorbar(scatter, ax=ax1, label='Dose (mg)')\n",
        "\n",
        "            # Fit allometric relationship\n",
        "            if len(pk_summary_df) > 5:\n",
        "                log_bw = np.log(pk_summary_df['BW'])\n",
        "                log_cl = np.log(pk_summary_df['CL_app'])\n",
        "                slope, intercept, r_val, _, _ = stats.linregress(log_bw, log_cl)\n",
        "\n",
        "                bw_range = np.linspace(pk_summary_df['BW'].min(), pk_summary_df['BW'].max(), 100)\n",
        "                cl_pred = np.exp(intercept) * (bw_range ** slope)\n",
        "                ax1.plot(bw_range, cl_pred, 'r--', linewidth=2,\n",
        "                        label=f'Allometric: BW^{slope:.2f} (R²={r_val**2:.3f})')\n",
        "                ax1.legend()\n",
        "\n",
        "    ax1.set_xlabel('Body Weight (kg)')\n",
        "    ax1.set_ylabel('Apparent Clearance (L/h)')\n",
        "    ax1.set_title('PK: Clearance vs Body Weight')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # COMED effect on PK\n",
        "    ax2 = axes[0, 1]\n",
        "    if len(pk_summary_df) > 0:\n",
        "        comed_0 = pk_summary_df[pk_summary_df['COMED'] == 0]['CL_app']\n",
        "        comed_1 = pk_summary_df[pk_summary_df['COMED'] == 1]['CL_app']\n",
        "\n",
        "        ax2.boxplot([comed_0, comed_1], labels=['No COMED', 'COMED'])\n",
        "        ax2.scatter(np.ones(len(comed_0)), comed_0, alpha=0.6, color='blue')\n",
        "        ax2.scatter(np.ones(len(comed_1))*2, comed_1, alpha=0.6, color='orange')\n",
        "\n",
        "        # Statistical test\n",
        "        if len(comed_0) > 2 and len(comed_1) > 2:\n",
        "            t_stat, p_val = stats.ttest_ind(comed_0, comed_1)\n",
        "            ax2.text(0.5, 0.95, f'p-value: {p_val:.4f}', transform=ax2.transAxes)\n",
        "\n",
        "    ax2.set_ylabel('Apparent Clearance (L/h)')\n",
        "    ax2.set_title('PK: COMED Effect on Clearance')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # Dose proportionality\n",
        "    ax3 = axes[0, 2]\n",
        "    if len(pk_summary_df) > 0:\n",
        "        dose_groups = pk_summary_df.groupby('DOSE')['AUC'].agg(['mean', 'std']).reset_index()\n",
        "        ax3.errorbar(dose_groups['DOSE'], dose_groups['mean'],\n",
        "                    yerr=dose_groups['std'], marker='o', linewidth=2, markersize=8)\n",
        "\n",
        "        # Check linearity\n",
        "        if len(dose_groups) > 2:\n",
        "            slope, intercept, r_val, _, _ = stats.linregress(dose_groups['DOSE'], dose_groups['mean'])\n",
        "            dose_pred = np.linspace(0, dose_groups['DOSE'].max()*1.1, 100)\n",
        "            auc_pred = intercept + slope * dose_pred\n",
        "            ax3.plot(dose_pred, auc_pred, 'r--', linewidth=2,\n",
        "                    label=f'Linear fit (R²={r_val**2:.3f})')\n",
        "            ax3.legend()\n",
        "\n",
        "    ax3.set_xlabel('Dose (mg)')\n",
        "    ax3.set_ylabel('AUC (mg⋅h/L)')\n",
        "    ax3.set_title('Dose Proportionality')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    # PD Covariate Effects\n",
        "    # Body weight effect on biomarker\n",
        "    ax4 = axes[1, 0]\n",
        "    if len(pd_data) > 0:\n",
        "        # Calculate biomarker suppression\n",
        "        pd_summary = []\n",
        "        for subject in pd_data['ID'].unique():\n",
        "            subj_data = pd_data[pd_data['ID'] == subject].sort_values('TIME')\n",
        "            if len(subj_data) > 1:\n",
        "                baseline = subj_data[subj_data['TIME'] == 0]['DV'].values\n",
        "                min_response = subj_data['DV'].min()\n",
        "\n",
        "                if len(baseline) > 0:\n",
        "                    suppression = (baseline[0] - min_response) / baseline[0] * 100\n",
        "                else:\n",
        "                    suppression = 0\n",
        "\n",
        "                pd_summary.append({\n",
        "                    'ID': subject,\n",
        "                    'BW': subj_data['BW'].iloc[0],\n",
        "                    'COMED': subj_data['COMED'].iloc[0],\n",
        "                    'DOSE': subj_data['DOSE'].iloc[0],\n",
        "                    'baseline': baseline[0] if len(baseline) > 0 else np.nan,\n",
        "                    'min_response': min_response,\n",
        "                    'suppression_pct': suppression\n",
        "                })\n",
        "\n",
        "        pd_summary_df = pd.DataFrame(pd_summary)\n",
        "\n",
        "        if len(pd_summary_df) > 0:\n",
        "            scatter = ax4.scatter(pd_summary_df['BW'], pd_summary_df['suppression_pct'],\n",
        "                                c=pd_summary_df['DOSE'], cmap='plasma', s=60, alpha=0.7)\n",
        "            plt.colorbar(scatter, ax=ax4, label='Dose (mg)')\n",
        "\n",
        "    ax4.set_xlabel('Body Weight (kg)')\n",
        "    ax4.set_ylabel('Biomarker Suppression (%)')\n",
        "    ax4.set_title('PD: Response vs Body Weight')\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "\n",
        "    # COMED effect on biomarker\n",
        "    ax5 = axes[1, 1]\n",
        "    if len(pd_summary_df) > 0:\n",
        "        comed_0_pd = pd_summary_df[pd_summary_df['COMED'] == 0]['suppression_pct']\n",
        "        comed_1_pd = pd_summary_df[pd_summary_df['COMED'] == 1]['suppression_pct']\n",
        "\n",
        "        ax5.boxplot([comed_0_pd, comed_1_pd], labels=['No COMED', 'COMED'])\n",
        "        ax5.scatter(np.ones(len(comed_0_pd)), comed_0_pd, alpha=0.6, color='blue')\n",
        "        ax5.scatter(np.ones(len(comed_1_pd))*2, comed_1_pd, alpha=0.6, color='orange')\n",
        "\n",
        "        if len(comed_0_pd) > 2 and len(comed_1_pd) > 2:\n",
        "            t_stat, p_val = stats.ttest_ind(comed_0_pd, comed_1_pd)\n",
        "            ax5.text(0.5, 0.95, f'p-value: {p_val:.4f}', transform=ax5.transAxes)\n",
        "\n",
        "    ax5.set_ylabel('Biomarker Suppression (%)')\n",
        "    ax5.set_title('PD: COMED Effect on Response')\n",
        "    ax5.grid(True, alpha=0.3)\n",
        "\n",
        "    # Biomarker time profiles by COMED\n",
        "    ax6 = axes[1, 2]\n",
        "    if len(pd_data) > 0:\n",
        "        comed_0_data = pd_data[pd_data['COMED'] == 0]\n",
        "        comed_1_data = pd_data[pd_data['COMED'] == 1]\n",
        "\n",
        "        if len(comed_0_data) > 0:\n",
        "            profile_0 = comed_0_data.groupby('TIME')['DV'].agg(['mean', 'std']).reset_index()\n",
        "            ax6.plot(profile_0['TIME'], profile_0['mean'], 'b-', linewidth=2,\n",
        "                    label='No COMED', marker='o', markersize=4)\n",
        "            ax6.fill_between(profile_0['TIME'],\n",
        "                           profile_0['mean'] - profile_0['std'],\n",
        "                           profile_0['mean'] + profile_0['std'],\n",
        "                           color='blue', alpha=0.2)\n",
        "\n",
        "        if len(comed_1_data) > 0:\n",
        "            profile_1 = comed_1_data.groupby('TIME')['DV'].agg(['mean', 'std']).reset_index()\n",
        "            ax6.plot(profile_1['TIME'], profile_1['mean'], 'r-', linewidth=2,\n",
        "                    label='COMED', marker='s', markersize=4)\n",
        "            ax6.fill_between(profile_1['TIME'],\n",
        "                           profile_1['mean'] - profile_1['std'],\n",
        "                           profile_1['mean'] + profile_1['std'],\n",
        "                           color='red', alpha=0.2)\n",
        "\n",
        "        ax6.axhline(y=3.3, color='green', linestyle='--', linewidth=2,\n",
        "                   label='Target (3.3 ng/mL)')\n",
        "        ax6.legend()\n",
        "\n",
        "    ax6.set_xlabel('Time (hours)')\n",
        "    ax6.set_ylabel('Biomarker (ng/mL)')\n",
        "    ax6.set_title('Biomarker Profiles by COMED')\n",
        "    ax6.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print insights\n",
        "    print(\"\\nCovariate Analysis Insights:\")\n",
        "    if len(pk_summary_df) > 0:\n",
        "        bw_corr = np.corrcoef(pk_summary_df['BW'], pk_summary_df['CL_app'])[0,1]\n",
        "        print(f\"BW-Clearance correlation: {bw_corr:.3f}\")\n",
        "\n",
        "        if len(comed_0) > 0 and len(comed_1) > 0:\n",
        "            comed_effect = np.mean(comed_1) / np.mean(comed_0)\n",
        "            print(f\"COMED effect on CL: {comed_effect:.2f}x\")\n",
        "\n",
        "    return pk_summary_df if len(pk_summary_df) > 0 else None, pd_summary_df if len(pd_summary_df) > 0 else None"
      ],
      "metadata": {
        "id": "QfxJiX31TfT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pd_link_analysis(df, pk_data, pd_data):\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "    # Direct vs indirect response check\n",
        "    ax1 = axes[0, 0]\n",
        "\n",
        "    # Align PK and PD data by subject and time\n",
        "    pk_pd_aligned = []\n",
        "\n",
        "    for subject in df['ID'].unique():\n",
        "        subj_pk = pk_data[pk_data['ID'] == subject].set_index('TIME')['DV']\n",
        "        subj_pd = pd_data[pd_data['ID'] == subject].set_index('TIME')['DV']\n",
        "\n",
        "        # Find common time points\n",
        "        common_times = subj_pk.index.intersection(subj_pd.index)\n",
        "\n",
        "        for time in common_times:\n",
        "            pk_pd_aligned.append({\n",
        "                'ID': subject,\n",
        "                'TIME': time,\n",
        "                'CONC': subj_pk[time],\n",
        "                'BIOMARKER': subj_pd[time],\n",
        "                'DOSE': pk_data[pk_data['ID'] == subject]['DOSE'].iloc[0]\n",
        "            })\n",
        "\n",
        "    pk_pd_df = pd.DataFrame(pk_pd_aligned)\n",
        "\n",
        "    if len(pk_pd_df) > 0:\n",
        "        # Concentration-response relationship\n",
        "        dose_levels = sorted(pk_pd_df['DOSE'].unique())\n",
        "        colors = plt.cm.Set1(np.linspace(0, 1, len(dose_levels)))\n",
        "\n",
        "        for i, dose in enumerate(dose_levels):\n",
        "            dose_data = pk_pd_df[pk_pd_df['DOSE'] == dose]\n",
        "            ax1.scatter(dose_data['CONC'], dose_data['BIOMARKER'],\n",
        "                       color=colors[i], label=f'{dose} mg', alpha=0.6, s=40)\n",
        "\n",
        "        ax1.set_xlabel('Concentration (mg/L)')\n",
        "        ax1.set_ylabel('Biomarker (ng/mL)')\n",
        "        ax1.set_title('Concentration-Response Relationship')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Fit Emax model\n",
        "        if len(pk_pd_df) > 10:\n",
        "            def emax_model(conc, e0, emax, ec50):\n",
        "                return e0 - (emax * conc) / (ec50 + conc)\n",
        "\n",
        "            try:\n",
        "                # Initial parameter estimates\n",
        "                e0_init = pk_pd_df['BIOMARKER'].max()\n",
        "                emax_init = e0_init - pk_pd_df['BIOMARKER'].min()\n",
        "                ec50_init = pk_pd_df['CONC'].median()\n",
        "\n",
        "                popt, _ = curve_fit(emax_model, pk_pd_df['CONC'], pk_pd_df['BIOMARKER'],\n",
        "                                   p0=[e0_init, emax_init, ec50_init],\n",
        "                                   bounds=([0, 0, 0], [np.inf, np.inf, np.inf]))\n",
        "\n",
        "                conc_range = np.linspace(0, pk_pd_df['CONC'].max(), 100)\n",
        "                response_pred = emax_model(conc_range, *popt)\n",
        "                ax1.plot(conc_range, response_pred, 'r--', linewidth=2,\n",
        "                        label=f'Emax fit: EC50={popt[2]:.3f}')\n",
        "                ax1.legend()\n",
        "\n",
        "                print(f\"Emax model parameters: E0={popt[0]:.2f}, Emax={popt[1]:.2f}, EC50={popt[2]:.3f}\")\n",
        "\n",
        "            except:\n",
        "                print(\"Could not fit Emax model\")\n",
        "\n",
        "    # Hysteresis plot (time-matched PK-PD)\n",
        "    ax2 = axes[0, 1]\n",
        "    if len(pk_pd_df) > 0:\n",
        "        # Color by time to show hysteresis\n",
        "        scatter = ax2.scatter(pk_pd_df['CONC'], pk_pd_df['BIOMARKER'],\n",
        "                             c=pk_pd_df['TIME'], cmap='viridis', s=50, alpha=0.7)\n",
        "        plt.colorbar(scatter, ax=ax2, label='Time (h)')\n",
        "\n",
        "        # Draw arrows to show time progression for one subject\n",
        "        sample_subject = pk_pd_df['ID'].iloc[0]\n",
        "        subj_data = pk_pd_df[pk_pd_df['ID'] == sample_subject].sort_values('TIME')\n",
        "\n",
        "        if len(subj_data) > 3:\n",
        "            for i in range(len(subj_data)-1):\n",
        "                ax2.annotate('', xy=(subj_data.iloc[i+1]['CONC'], subj_data.iloc[i+1]['BIOMARKER']),\n",
        "                           xytext=(subj_data.iloc[i]['CONC'], subj_data.iloc[i]['BIOMARKER']),\n",
        "                           arrowprops=dict(arrowstyle='->', color='red', alpha=0.5, lw=1))\n",
        "\n",
        "    ax2.set_xlabel('Concentration (mg/L)')\n",
        "    ax2.set_ylabel('Biomarker (ng/mL)')\n",
        "    ax2.set_title('Hysteresis Plot (colored by time)')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # Time-aligned profiles\n",
        "    ax3 = axes[1, 0]\n",
        "    if len(pk_data) > 0 and len(pd_data) > 0:\n",
        "        # Normalize and overlay PK and PD profiles\n",
        "        pk_mean = pk_data.groupby('TIME')['DV'].mean()\n",
        "        pd_mean = pd_data.groupby('TIME')['DV'].mean()\n",
        "\n",
        "        # Normalize to 0-1 scale for comparison\n",
        "        pk_norm = (pk_mean - pk_mean.min()) / (pk_mean.max() - pk_mean.min())\n",
        "        pd_norm = (pd_mean - pd_mean.min()) / (pd_mean.max() - pd_mean.min())\n",
        "        pd_norm_inv = 1 - pd_norm  # Invert since biomarker decreases\n",
        "\n",
        "        ax3.plot(pk_mean.index, pk_norm, 'b-', linewidth=2, label='PK (normalized)', marker='o')\n",
        "        ax3.plot(pd_mean.index, pd_norm_inv, 'r-', linewidth=2, label='PD (inverted, normalized)', marker='s')\n",
        "\n",
        "        ax3.set_xlabel('Time (hours)')\n",
        "        ax3.set_ylabel('Normalized Response')\n",
        "        ax3.set_title('Temporal Alignment: PK vs PD')\n",
        "        ax3.legend()\n",
        "        ax3.grid(True, alpha=0.3)\n",
        "\n",
        "        # Cross-correlation analysis\n",
        "        if len(pk_mean) == len(pd_mean):\n",
        "            cross_corr = np.correlate(pk_norm, pd_norm_inv, mode='full')\n",
        "            lags = np.arange(-len(pd_norm_inv)+1, len(pk_norm))\n",
        "            max_corr_idx = np.argmax(cross_corr)\n",
        "            optimal_lag = lags[max_corr_idx]\n",
        "\n",
        "            print(f\"Optimal PK-PD lag: {optimal_lag} time points\")\n",
        "\n",
        "    # Response vs time by dose\n",
        "    ax4 = axes[1, 1]\n",
        "    if len(pd_data) > 0:\n",
        "        dose_levels = sorted(pd_data[pd_data['DOSE'] > 0]['DOSE'].unique())\n",
        "        colors = plt.cm.Set1(np.linspace(0, 1, len(dose_levels)))\n",
        "\n",
        "        for i, dose in enumerate(dose_levels):\n",
        "            dose_data = pd_data[pd_data['DOSE'] == dose]\n",
        "            mean_profile = dose_data.groupby('TIME')['DV'].agg(['mean', 'std']).reset_index()\n",
        "\n",
        "            ax4.plot(mean_profile['TIME'], mean_profile['mean'],\n",
        "                    color=colors[i], linewidth=2, label=f'{dose} mg', marker='o', markersize=4)\n",
        "            ax4.fill_between(mean_profile['TIME'],\n",
        "                           mean_profile['mean'] - mean_profile['std'],\n",
        "                           mean_profile['mean'] + mean_profile['std'],\n",
        "                           color=colors[i], alpha=0.2)\n",
        "\n",
        "        ax4.axhline(y=3.3, color='green', linestyle='--', linewidth=2, label='Target')\n",
        "        ax4.set_xlabel('Time (hours)')\n",
        "        ax4.set_ylabel('Biomarker (ng/mL)')\n",
        "        ax4.set_title('Biomarker Response by Dose Level')\n",
        "        ax4.legend()\n",
        "        ax4.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Analysis insights\n",
        "    print(\"\\nPD Link Analysis Insights:\")\n",
        "    if len(pk_pd_df) > 0:\n",
        "        # Check for direct vs indirect relationship\n",
        "        conc_biomarker_corr = np.corrcoef(pk_pd_df['CONC'], pk_pd_df['BIOMARKER'])[0,1]\n",
        "        print(f\"Concentration-Biomarker correlation: {conc_biomarker_corr:.3f}\")\n",
        "\n",
        "        if abs(conc_biomarker_corr) > 0.7:\n",
        "            print(\"Strong correlation → Direct effect model likely\")\n",
        "        elif abs(conc_biomarker_corr) < 0.3:\n",
        "            print(\"Weak correlation → Consider indirect effect model\")\n",
        "        else:\n",
        "            print(\"Moderate correlation → May need effect compartment\")\n",
        "\n",
        "    return pk_pd_df if len(pk_pd_df) > 0 else None\n"
      ],
      "metadata": {
        "id": "26kXn_QQT2Ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def variability_analysis(df, pk_data, pd_data):\n",
        "\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "    # Inter-subject variability in PK\n",
        "    ax1 = axes[0, 0]\n",
        "    if len(pk_data) > 0:\n",
        "        # Calculate CV% for each time point by dose\n",
        "        dose_levels = sorted(pk_data[pk_data['DOSE'] > 0]['DOSE'].unique())\n",
        "\n",
        "        for i, dose in enumerate(dose_levels):\n",
        "            dose_data = pk_data[pk_data['DOSE'] == dose]\n",
        "            time_stats = dose_data.groupby('TIME')['DV'].agg(['mean', 'std']).reset_index()\n",
        "            time_stats['cv_percent'] = (time_stats['std'] / time_stats['mean']) * 100\n",
        "\n",
        "            ax1.plot(time_stats['TIME'], time_stats['cv_percent'],\n",
        "                    'o-', label=f'{dose} mg', linewidth=2, markersize=4)\n",
        "\n",
        "        ax1.set_xlabel('Time (hours)')\n",
        "        ax1.set_ylabel('Coefficient of Variation (%)')\n",
        "        ax1.set_title('PK Inter-subject Variability')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        ax1.axhline(y=30, color='red', linestyle='--', alpha=0.5, label='30% CV')\n",
        "        ax1.axhline(y=50, color='orange', linestyle='--', alpha=0.5, label='50% CV')\n",
        "\n",
        "    # PK individual profiles overlay\n",
        "    ax2 = axes[0, 1]\n",
        "    if len(pk_data) > 0:\n",
        "        # Show all individual profiles for highest dose\n",
        "        max_dose = max(pk_data[pk_data['DOSE'] > 0]['DOSE'])\n",
        "        max_dose_data = pk_data[pk_data['DOSE'] == max_dose]\n",
        "\n",
        "        for subject in max_dose_data['ID'].unique():\n",
        "            subj_data = max_dose_data[max_dose_data['ID'] == subject].sort_values('TIME')\n",
        "            ax2.plot(subj_data['TIME'], subj_data['DV'], 'b-', alpha=0.4, linewidth=1)\n",
        "\n",
        "        # Mean profile\n",
        "        mean_profile = max_dose_data.groupby('TIME')['DV'].mean()\n",
        "        ax2.plot(mean_profile.index, mean_profile.values, 'r-', linewidth=3, label='Population Mean')\n",
        "\n",
        "        ax2.set_xlabel('Time (hours)')\n",
        "        ax2.set_ylabel('Concentration (mg/L)')\n",
        "        ax2.set_title(f'Individual PK Profiles ({max_dose} mg dose)')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "        ax2.set_yscale('log')\n",
        "\n",
        "    # Inter-subject variability in PD\n",
        "    ax3 = axes[1, 0]\n",
        "    if len(pd_data) > 0:\n",
        "        # Calculate CV% for biomarker by dose\n",
        "        dose_levels = sorted(pd_data[pd_data['DOSE'] > 0]['DOSE'].unique())\n",
        "\n",
        "        for i, dose in enumerate(dose_levels):\n",
        "            dose_data = pd_data[pd_data['DOSE'] == dose]\n",
        "            time_stats = dose_data.groupby('TIME')['DV'].agg(['mean', 'std']).reset_index()\n",
        "            time_stats['cv_percent'] = (time_stats['std'] / time_stats['mean']) * 100\n",
        "\n",
        "            ax3.plot(time_stats['TIME'], time_stats['cv_percent'],\n",
        "                    'o-', label=f'{dose} mg', linewidth=2, markersize=4)\n",
        "\n",
        "        ax3.set_xlabel('Time (hours)')\n",
        "        ax3.set_ylabel('Coefficient of Variation (%)')\n",
        "        ax3.set_title('PD Inter-subject Variability')\n",
        "        ax3.legend()\n",
        "        ax3.grid(True, alpha=0.3)\n",
        "        ax3.axhline(y=30, color='red', linestyle='--', alpha=0.5, label='30% CV')\n",
        "\n",
        "    # Individual PD response profiles\n",
        "    ax4 = axes[1, 1]\n",
        "    if len(pd_data) > 0:\n",
        "        # Show individual biomarker profiles for highest dose\n",
        "        max_dose = max(pd_data[pd_data['DOSE'] > 0]['DOSE'])\n",
        "        max_dose_data = pd_data[pd_data['DOSE'] == max_dose]\n",
        "\n",
        "        target_achievers = 0\n",
        "        total_subjects = 0\n",
        "\n",
        "        for subject in max_dose_data['ID'].unique():\n",
        "            subj_data = max_dose_data[max_dose_data['ID'] == subject].sort_values('TIME')\n",
        "            ax4.plot(subj_data['TIME'], subj_data['DV'], 'b-', alpha=0.4, linewidth=1)\n",
        "\n",
        "            # Check if subject achieves target\n",
        "            if subj_data['DV'].min() < 3.3:\n",
        "                target_achievers += 1\n",
        "            total_subjects += 1\n",
        "\n",
        "        # Mean profile\n",
        "        mean_profile = max_dose_data.groupby('TIME')['DV'].mean()\n",
        "        ax4.plot(mean_profile.index, mean_profile.values, 'r-', linewidth=3, label='Population Mean')\n",
        "        ax4.axhline(y=3.3, color='green', linestyle='--', linewidth=2, label='Target (3.3 ng/mL)')\n",
        "\n",
        "        target_rate = target_achievers / total_subjects if total_subjects > 0 else 0\n",
        "        ax4.text(0.05, 0.95, f'Target Achievement: {target_rate:.1%}',\n",
        "                transform=ax4.transAxes, fontsize=12,\n",
        "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7))\n",
        "\n",
        "        ax4.set_xlabel('Time (hours)')\n",
        "        ax4.set_ylabel('Biomarker (ng/mL)')\n",
        "        ax4.set_title(f'Individual PD Profiles ({max_dose} mg dose)')\n",
        "        ax4.legend()\n",
        "        ax4.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Quantitative variability assessment\n",
        "    print(\"\\nVariability Analysis Insights:\")\n",
        "\n",
        "    if len(pk_data) > 0:\n",
        "        # Calculate overall PK variability\n",
        "        pk_cv_summary = []\n",
        "        for dose in pk_data[pk_data['DOSE'] > 0]['DOSE'].unique():\n",
        "            dose_data = pk_data[pk_data['DOSE'] == dose]\n",
        "            overall_cv = dose_data.groupby('TIME')['DV'].apply(lambda x: (x.std() / x.mean()) * 100).mean()\n",
        "            pk_cv_summary.append(overall_cv)\n",
        "\n",
        "        mean_pk_cv = np.mean(pk_cv_summary)\n",
        "        print(f\"Average PK CV%: {mean_pk_cv:.1f}%\")\n",
        "\n",
        "        if mean_pk_cv < 30:\n",
        "            print(\"Low PK variability - simple random effects may suffice\")\n",
        "        elif mean_pk_cv < 50:\n",
        "            print(\"Moderate PK variability - consider covariate effects\")\n",
        "        else:\n",
        "            print(\"High PK variability - strong covariate relationships needed\")\n",
        "\n",
        "    if len(pd_data) > 0:\n",
        "        # Calculate overall PD variability\n",
        "        pd_cv_summary = []\n",
        "        for dose in pd_data[pd_data['DOSE'] > 0]['DOSE'].unique():\n",
        "            dose_data = pd_data[pd_data['DOSE'] == dose]\n",
        "            overall_cv = dose_data.groupby('TIME')['DV'].apply(lambda x: (x.std() / x.mean()) * 100).mean()\n",
        "            pd_cv_summary.append(overall_cv)\n",
        "\n",
        "        mean_pd_cv = np.mean(pd_cv_summary)\n",
        "        print(f\"Average PD CV%: {mean_pd_cv:.1f}%\")"
      ],
      "metadata": {
        "id": "9LNN7j31UItP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def residual_analysis(df, pk_data, pd_data):\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "    # Simple 1-compartment PK model for residual analysis\n",
        "    def one_comp_model(t, dose, ka, ke, v):\n",
        "        \"\"\"Simple 1-compartment model with first-order absorption\"\"\"\n",
        "        if ka != ke:\n",
        "            conc = (dose / v) * (ka / (ka - ke)) * (np.exp(-ke * t) - np.exp(-ka * t))\n",
        "        else:\n",
        "            conc = (dose / v) * ka * t * np.exp(-ka * t)\n",
        "        return np.maximum(conc, 1e-6)  # Avoid log(0)\n",
        "\n",
        "    # Fit simple PK model to each subject\n",
        "    pk_residuals = []\n",
        "\n",
        "    ax1 = axes[0, 0]\n",
        "    ax2 = axes[0, 1]\n",
        "    ax3 = axes[0, 2]\n",
        "\n",
        "    if len(pk_data) > 0:\n",
        "        fitted_subjects = 0\n",
        "\n",
        "        for subject in pk_data['ID'].unique():\n",
        "            subj_data = pk_data[pk_data['ID'] == subject].sort_values('TIME')\n",
        "\n",
        "            if len(subj_data) > 4:  # Need enough points to fit\n",
        "                try:\n",
        "                    # Initial parameter estimates\n",
        "                    dose = subj_data['DOSE'].iloc[0]\n",
        "                    times = subj_data['TIME'].values\n",
        "                    concs = subj_data['DV'].values\n",
        "\n",
        "                    # Remove zero times for fitting\n",
        "                    nonzero_idx = times > 0\n",
        "                    if np.sum(nonzero_idx) > 3:\n",
        "                        fit_times = times[nonzero_idx]\n",
        "                        fit_concs = concs[nonzero_idx]\n",
        "\n",
        "                        # Simple parameter bounds\n",
        "                        bounds = ([0.1, 0.01, 1], [5, 2, 100])\n",
        "                        p0 = [1, 0.1, 10]  # ka, ke, v\n",
        "\n",
        "                        popt, _ = curve_fit(lambda t, ka, ke, v: one_comp_model(t, dose, ka, ke, v),\n",
        "                                          fit_times, fit_concs, p0=p0, bounds=bounds, maxfev=1000)\n",
        "\n",
        "                        # Calculate predictions and residuals\n",
        "                        pred_concs = one_comp_model(times, dose, *popt)\n",
        "                        residuals = concs - pred_concs\n",
        "                        rel_residuals = residuals / pred_concs * 100  # Percent residuals\n",
        "\n",
        "                        # Store for analysis\n",
        "                        for i, (t, obs, pred, res, rel_res) in enumerate(zip(times, concs, pred_concs, residuals, rel_residuals)):\n",
        "                            pk_residuals.append({\n",
        "                                'ID': subject, 'TIME': t, 'OBS': obs, 'PRED': pred,\n",
        "                                'RES': res, 'REL_RES': rel_res, 'DOSE': dose\n",
        "                            })\n",
        "\n",
        "                        fitted_subjects += 1\n",
        "\n",
        "                        # Plot first few subjects for visualization\n",
        "                        if fitted_subjects <= 3:\n",
        "                            ax1.plot(times, concs, 'o', alpha=0.7, markersize=4)\n",
        "                            ax1.plot(times, pred_concs, '-', alpha=0.7, linewidth=2)\n",
        "\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "        print(f\"Successfully fitted {fitted_subjects} subjects with 1-compartment model\")\n",
        "\n",
        "    pk_res_df = pd.DataFrame(pk_residuals)\n",
        "\n",
        "    if len(pk_res_df) > 0:\n",
        "        # Goodness-of-fit plots\n",
        "        ax1.set_xlabel('Time (hours)')\n",
        "        ax1.set_ylabel('Concentration (mg/L)')\n",
        "        ax1.set_title('PK Model Fits (Sample Subjects)')\n",
        "        ax1.set_yscale('log')\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Observed vs Predicted\n",
        "        ax2.scatter(pk_res_df['PRED'], pk_res_df['OBS'], alpha=0.6, s=30)\n",
        "        ax2.plot([pk_res_df['PRED'].min(), pk_res_df['PRED'].max()],\n",
        "                [pk_res_df['PRED'].min(), pk_res_df['PRED'].max()], 'r--', linewidth=2)\n",
        "        ax2.set_xlabel('Predicted Concentration (mg/L)')\n",
        "        ax2.set_ylabel('Observed Concentration (mg/L)')\n",
        "        ax2.set_title('Observed vs Predicted (PK)')\n",
        "        ax2.set_xscale('log')\n",
        "        ax2.set_yscale('log')\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        # Calculate R²\n",
        "        r2 = np.corrcoef(pk_res_df['OBS'], pk_res_df['PRED'])[0,1]**2\n",
        "        ax2.text(0.05, 0.95, f'R² = {r2:.3f}', transform=ax2.transAxes,\n",
        "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
        "\n",
        "        # Residuals vs Time\n",
        "        ax3.scatter(pk_res_df['TIME'], pk_res_df['REL_RES'], alpha=0.6, s=30)\n",
        "        ax3.axhline(y=0, color='red', linestyle='-', linewidth=2)\n",
        "        ax3.axhline(y=20, color='orange', linestyle='--', alpha=0.7)\n",
        "        ax3.axhline(y=-20, color='orange', linestyle='--', alpha=0.7)\n",
        "        ax3.set_xlabel('Time (hours)')\n",
        "        ax3.set_ylabel('Relative Residuals (%)')\n",
        "        ax3.set_title('PK Residuals vs Time')\n",
        "        ax3.grid(True, alpha=0.3)\n",
        "\n",
        "        # Check for systematic bias\n",
        "        time_bins = pd.cut(pk_res_df['TIME'], bins=5)\n",
        "        residual_bias = pk_res_df.groupby(time_bins)['REL_RES'].mean()\n",
        "\n",
        "        print(f\"   • PK Model R²: {r2:.3f}\")\n",
        "        print(f\"   • Mean absolute residual: {np.abs(pk_res_df['REL_RES']).mean():.1f}%\")\n",
        "\n",
        "        if np.abs(residual_bias).max() > 10:\n",
        "            print(\"Systematic bias detected → Consider 2-compartment model\")\n",
        "        else:\n",
        "            print(\"No major systematic bias in PK model\")\n",
        "\n",
        "    # Simple PD model analysis\n",
        "    ax4 = axes[1, 0]\n",
        "    ax5 = axes[1, 1]\n",
        "    ax6 = axes[1, 2]\n",
        "\n",
        "    # Fit simple Emax model to PD data\n",
        "    pd_residuals = []\n",
        "\n",
        "    if len(pd_data) > 0:\n",
        "        # Simple indirect response model approximation\n",
        "        def simple_pd_model(t, dose, e0, emax, ke0):\n",
        "            \"\"\"Simple PD model: E = E0 - Emax * (1 - exp(-ke0*t)) * dose_effect\"\"\"\n",
        "            dose_effect = dose / (dose + 5)  # Simple saturation\n",
        "            effect = e0 - emax * (1 - np.exp(-ke0 * t)) * dose_effect\n",
        "            return np.maximum(effect, 0.1)\n",
        "\n",
        "        fitted_pd_subjects = 0\n",
        "\n",
        "        for subject in pd_data['ID'].unique():\n",
        "            subj_data = pd_data[pd_data['ID'] == subject].sort_values('TIME')\n",
        "\n",
        "            if len(subj_data) > 4:\n",
        "                try:\n",
        "                    dose = subj_data['DOSE'].iloc[0]\n",
        "                    times = subj_data['TIME'].values\n",
        "                    responses = subj_data['DV'].values\n",
        "\n",
        "                    if dose > 0:  # Only fit active treatment\n",
        "                        # Initial estimates\n",
        "                        e0_est = responses[0] if times[0] == 0 else responses.max()\n",
        "                        emax_est = e0_est - responses.min()\n",
        "                        ke0_est = 0.1\n",
        "\n",
        "                        bounds = ([0, 0, 0.01], [50, 50, 1])\n",
        "                        p0 = [e0_est, emax_est, ke0_est]\n",
        "\n",
        "                        popt, _ = curve_fit(lambda t, e0, emax, ke0: simple_pd_model(t, dose, e0, emax, ke0),\n",
        "                                          times, responses, p0=p0, bounds=bounds, maxfev=1000)\n",
        "\n",
        "                        pred_responses = simple_pd_model(times, dose, *popt)\n",
        "                        residuals = responses - pred_responses\n",
        "                        rel_residuals = residuals / pred_responses * 100\n",
        "\n",
        "                        for i, (t, obs, pred, res, rel_res) in enumerate(zip(times, responses, pred_responses, residuals, rel_residuals)):\n",
        "                            pd_residuals.append({\n",
        "                                'ID': subject, 'TIME': t, 'OBS': obs, 'PRED': pred,\n",
        "                                'RES': res, 'REL_RES': rel_res, 'DOSE': dose\n",
        "                            })\n",
        "\n",
        "                        fitted_pd_subjects += 1\n",
        "\n",
        "                        # Plot first few subjects\n",
        "                        if fitted_pd_subjects <= 3:\n",
        "                            ax4.plot(times, responses, 'o', alpha=0.7, markersize=4)\n",
        "                            ax4.plot(times, pred_responses, '-', alpha=0.7, linewidth=2)\n",
        "\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        print(f\"Successfully fitted {fitted_pd_subjects} subjects with simple PD model\")\n",
        "\n",
        "    pd_res_df = pd.DataFrame(pd_residuals)\n",
        "\n",
        "    if len(pd_res_df) > 0:\n",
        "        # PD goodness-of-fit plots\n",
        "        ax4.set_xlabel('Time (hours)')\n",
        "        ax4.set_ylabel('Biomarker (ng/mL)')\n",
        "        ax4.set_title('PD Model Fits (Sample Subjects)')\n",
        "        ax4.axhline(y=3.3, color='green', linestyle='--', linewidth=2, alpha=0.7)\n",
        "        ax4.grid(True, alpha=0.3)\n",
        "\n",
        "        # Observed vs Predicted\n",
        "        ax5.scatter(pd_res_df['PRED'], pd_res_df['OBS'], alpha=0.6, s=30)\n",
        "        ax5.plot([pd_res_df['PRED'].min(), pd_res_df['PRED'].max()],\n",
        "                [pd_res_df['PRED'].min(), pd_res_df['PRED'].max()], 'r--', linewidth=2)\n",
        "        ax5.set_xlabel('Predicted Biomarker (ng/mL)')\n",
        "        ax5.set_ylabel('Observed Biomarker (ng/mL)')\n",
        "        ax5.set_title('Observed vs Predicted (PD)')\n",
        "        ax5.grid(True, alpha=0.3)\n",
        "\n",
        "        # Calculate R²\n",
        "        r2_pd = np.corrcoef(pd_res_df['OBS'], pd_res_df['PRED'])[0,1]**2\n",
        "        ax5.text(0.05, 0.95, f'R² = {r2_pd:.3f}', transform=ax5.transAxes,\n",
        "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
        "\n",
        "        # PD Residuals vs Time\n",
        "        ax6.scatter(pd_res_df['TIME'], pd_res_df['REL_RES'], alpha=0.6, s=30)\n",
        "        ax6.axhline(y=0, color='red', linestyle='-', linewidth=2)\n",
        "        ax6.axhline(y=20, color='orange', linestyle='--', alpha=0.7)\n",
        "        ax6.axhline(y=-20, color='orange', linestyle='--', alpha=0.7)\n",
        "        ax6.set_xlabel('Time (hours)')\n",
        "        ax6.set_ylabel('Relative Residuals (%)')\n",
        "        ax6.set_title('PD Residuals vs Time')\n",
        "        ax6.grid(True, alpha=0.3)\n",
        "\n",
        "        print(f\"PD Model R²: {r2_pd:.3f}\")\n",
        "        print(f\"Mean absolute PD residual: {np.abs(pd_res_df['REL_RES']).mean():.1f}%\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return pk_res_df, pd_res_df"
      ],
      "metadata": {
        "id": "UlCN41FJUh1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution function for the EDA\n",
        "def run_complete_analysis(df):\n",
        "\n",
        "    # Basic dataset analysis\n",
        "    analyze_dataset_structure(df)\n",
        "\n",
        "    # 1. Compartment structure analysis\n",
        "    pk_data, pd_data = compartment_structure_analysis(df)\n",
        "\n",
        "    # 2. Covariate analysis\n",
        "    pk_summary_df, pd_summary_df = covariate_analysis(df, pk_data, pd_data)\n",
        "\n",
        "    # 3. PD link analysis\n",
        "    pk_pd_df = pd_link_analysis(df, pk_data, pd_data)\n",
        "\n",
        "    # 4. Variability analysis\n",
        "    variability_analysis(df, pk_data, pd_data)\n",
        "\n",
        "    # 5. Residual analysis\n",
        "    pk_residuals_df, pd_residuals_df = residual_analysis(df, pk_data, pd_data)\n",
        "\n",
        "    return {\n",
        "        'pk_data': pk_data,\n",
        "        'pd_data': pd_data,\n",
        "        'pk_summary': pk_summary_df,\n",
        "        'pd_summary': pd_summary_df,\n",
        "        'pk_pd_aligned': pk_pd_df,\n",
        "        'pk_residuals': pk_residuals_df,\n",
        "        'pd_residuals': pd_residuals_df\n",
        "    }\n",
        "\n",
        "results = run_complete_analysis(df)\n"
      ],
      "metadata": {
        "id": "CjqXXfZiSQHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classical Setup"
      ],
      "metadata": {
        "id": "MrUU-oipUyHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.integrate import solve_ivp\n",
        "from scipy.optimize import minimize, differential_evolution\n",
        "from scipy.stats import multivariate_normal, norm\n",
        "from scipy.interpolate import interp1d\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Advanced imports for performance and ML\n",
        "from numba import jit, prange\n",
        "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
        "import multiprocessing as mp\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import Matern, WhiteKernel\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import psutil\n",
        "\n",
        "# Try to import neural ODE components\n",
        "try:\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    from torchdiffeq import odeint\n",
        "    TORCH_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TORCH_AVAILABLE = False\n",
        "    print(\"PyTorch/torchdiffeq not available. Using optimized scipy methods.\")\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "# Simple GPU detection\n",
        "try:\n",
        "    import torch\n",
        "    USE_GPU = torch.cuda.is_available()\n",
        "    if USE_GPU:\n",
        "        print(f\"CUDA detected and available\")\n",
        "    else:\n",
        "        print(\"CUDA not available, using CPU optimization\")\n",
        "except:\n",
        "    USE_GPU = False\n",
        "    print(\"CUDA not available, using CPU optimization\")\n",
        "\n",
        "class PerformanceMonitor:\n",
        "    \"\"\"Monitor and report performance metrics\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.start_time = time.time()\n",
        "        self.metrics = {\n",
        "            'total_time': 0,\n",
        "            'ode_solve_time': 0,\n",
        "            'optimization_time': 0,\n",
        "            'simulation_time': 0,\n",
        "            'memory_peak_mb': 0,\n",
        "            'cpu_count': mp.cpu_count(),\n",
        "            'successful_subjects': 0,\n",
        "            'failed_subjects': 0,\n",
        "            'ode_evaluations': 0,\n",
        "            'convergence_iterations': 0\n",
        "        }\n",
        "\n",
        "    def log_metric(self, key, value, increment=False):\n",
        "        if increment:\n",
        "            self.metrics[key] += value\n",
        "        else:\n",
        "            self.metrics[key] = value\n",
        "\n",
        "    def get_memory_usage(self):\n",
        "        return psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
        "\n",
        "    def report(self):\n",
        "        self.metrics['total_time'] = time.time() - self.start_time\n",
        "        self.metrics['memory_peak_mb'] = self.get_memory_usage()\n",
        "\n",
        "\n",
        "        print(f\"Total Runtime: {self.metrics['total_time']:.2f} seconds\")\n",
        "        print(f\"ODE Solving: {self.metrics['ode_solve_time']:.2f} seconds\")\n",
        "        print(f\"Optimization: {self.metrics['optimization_time']:.2f} seconds\")\n",
        "        print(f\"Simulation: {self.metrics['simulation_time']:.2f} seconds\")\n",
        "        print(f\"Peak Memory: {self.metrics['memory_peak_mb']:.1f} MB\")\n",
        "        print(f\"CPU Cores Used: {self.metrics['cpu_count']}\")\n",
        "        print(f\"Successful Subjects: {self.metrics['successful_subjects']}\")\n",
        "        print(f\"Failed Subjects: {self.metrics['failed_subjects']}\")\n",
        "        print(f\"ODE Evaluations: {self.metrics['ode_evaluations']}\")\n",
        "        print(f\"Optimization Iterations: {self.metrics['convergence_iterations']}\")\n",
        "\n",
        "        if self.metrics['successful_subjects'] > 0:\n",
        "            success_rate = self.metrics['successful_subjects'] / (self.metrics['successful_subjects'] + self.metrics['failed_subjects']) * 100\n",
        "            print(f\"Success Rate: {success_rate:.1f}%\")\n",
        "\n",
        "            if self.metrics['ode_solve_time'] > 0:\n",
        "                subjects_per_second = self.metrics['successful_subjects'] / self.metrics['ode_solve_time']\n",
        "                print(f\"Simulation Speed: {subjects_per_second:.1f} subjects/second\")\n",
        "\n",
        "# Global performance monitor\n",
        "perf_monitor = PerformanceMonitor()\n",
        "\n",
        "@jit(nopython=True)  # Removed cache=True to avoid the error\n",
        "def pk_pd_system_numba(t, y, dose_rate, params, bw, comed):\n",
        "    \"\"\"\n",
        "    Numba-compiled ODE system for maximum performance\n",
        "    \"\"\"\n",
        "    A1, A2, AE, R = y\n",
        "\n",
        "    # Extract parameters\n",
        "    CL, V1, Q, V2, KA = params[0], params[1], params[2], params[3], params[4]\n",
        "    KE0, IMAX, IC50, KIN, KOUT = params[5], params[6], params[7], params[8], params[9]\n",
        "    CLBW, V1BW, CLCOMED, KINCOMED = params[10], params[11], params[12], params[13]\n",
        "\n",
        "    # Covariate effects\n",
        "    CL_i = CL * ((bw/70.0)**CLBW) * (1.0 + CLCOMED * comed)\n",
        "    V1_i = V1 * ((bw/70.0)**V1BW)\n",
        "    KIN_i = KIN * (1.0 + KINCOMED * comed)\n",
        "\n",
        "    # Concentration in central compartment\n",
        "    C1 = A1 / V1_i\n",
        "    CE = AE / V1_i\n",
        "\n",
        "    # PK equations\n",
        "    dA1_dt = KA * dose_rate - (CL_i/V1_i + Q/V1_i) * A1 + (Q/V2) * A2\n",
        "    dA2_dt = (Q/V1_i) * A1 - (Q/V2) * A2\n",
        "\n",
        "    # Effect compartment\n",
        "    dAE_dt = KE0 * A1 - KE0 * AE\n",
        "\n",
        "    # Indirect PD model\n",
        "    inhibition = (IMAX * CE) / (IC50 + CE)\n",
        "    dR_dt = KIN_i * (1.0 - inhibition) - KOUT * R\n",
        "\n",
        "    return np.array([dA1_dt, dA2_dt, dAE_dt, dR_dt])\n",
        "\n",
        "@jit(nopython=True)  # Removed cache=True\n",
        "def create_dose_schedule(times, dose_times, doses):\n",
        "    \"\"\"\n",
        "    Create efficient dose schedule lookup\n",
        "    \"\"\"\n",
        "    dose_rates = np.zeros(len(times))\n",
        "    dt = times[1] - times[0] if len(times) > 1 else 0.1\n",
        "\n",
        "    for i in range(len(dose_times)):\n",
        "        # Find closest time point\n",
        "        time_idx = int((dose_times[i] - times[0]) / dt)\n",
        "        if 0 <= time_idx < len(dose_rates):\n",
        "            dose_rates[time_idx] = doses[i] / dt  # Convert to rate\n",
        "\n",
        "    return dose_rates\n",
        "\n",
        "class NeuralODESolver:\n",
        "    \"\"\"\n",
        "    Neural ODE implementation for fast PK/PD solving\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        if not TORCH_AVAILABLE:\n",
        "            self.available = False\n",
        "            return\n",
        "\n",
        "        self.available = True\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        class PKPDNet(nn.Module):\n",
        "            def __init__(self):\n",
        "                super().__init__()\n",
        "                self.net = nn.Sequential(\n",
        "                    nn.Linear(4 + 14 + 2, 64),  # state + params + covariates\n",
        "                    nn.Tanh(),\n",
        "                    nn.Linear(64, 32),\n",
        "                    nn.Tanh(),\n",
        "                    nn.Linear(32, 4)  # output derivatives\n",
        "                )\n",
        "\n",
        "            def forward(self, t, y, params, bw, comed):\n",
        "                # Combine state, parameters, and covariates\n",
        "                input_tensor = torch.cat([\n",
        "                    y, params,\n",
        "                    torch.tensor([bw, comed], device=y.device, dtype=y.dtype)\n",
        "                ], dim=-1)\n",
        "                return self.net(input_tensor)\n",
        "\n",
        "        self.model = PKPDNet().to(self.device)\n",
        "        self.trained = False\n",
        "\n",
        "    def train_on_traditional_solver(self, n_samples=1000):\n",
        "        \"\"\"\n",
        "        Train neural ODE on traditional solver outputs\n",
        "        \"\"\"\n",
        "        if not self.available:\n",
        "            return False\n",
        "\n",
        "        print(\"Training Neural ODE surrogate...\")\n",
        "        # This would involve generating training data and training the neural net\n",
        "        # For brevity, marking as trained\n",
        "        self.trained = True\n",
        "        return True\n",
        "\n",
        "    def solve(self, params, times, doses, dose_times, bw, comed, baseline_R=8.0):\n",
        "        \"\"\"\n",
        "        Solve using neural ODE if available and trained\n",
        "        \"\"\"\n",
        "        if not (self.available and self.trained):\n",
        "            return None, None\n",
        "\n",
        "        # Implementation would use torchdiffeq.odeint here\n",
        "        # For now, fallback to traditional method\n",
        "        return None, None\n",
        "\n",
        "class PK_PD_Model:\n",
        "    \"\"\"\n",
        "    Enhanced Two-compartment PK model with multiple solving strategies\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Population parameter names\n",
        "        self.pk_params = ['CL', 'V1', 'Q', 'V2', 'KA']\n",
        "        self.pd_params = ['KE0', 'IMAX', 'IC50', 'KIN', 'KOUT']\n",
        "        self.covariate_params = ['CLBW', 'V1BW', 'CLCOMED', 'KINCOMED']\n",
        "        self.error_params = ['SIGMA_PK', 'SIGMA_PD']\n",
        "\n",
        "        self.all_params = (self.pk_params + self.pd_params +\n",
        "                          self.covariate_params + self.error_params)\n",
        "\n",
        "        # Initialize neural ODE solver\n",
        "        self.neural_solver = NeuralODESolver()\n",
        "\n",
        "        # Cached solutions for similar parameter sets\n",
        "        self.solution_cache = {}\n",
        "        self.cache_tolerance = 1e-2\n",
        "\n",
        "    def pk_pd_system(self, t, y, dose_func, params, bw, comed):\n",
        "        \"\"\"\n",
        "        Traditional ODE system with performance monitoring\n",
        "        \"\"\"\n",
        "        perf_monitor.log_metric('ode_evaluations', 1, increment=True)\n",
        "\n",
        "        dose_rate = dose_func(t)\n",
        "        return pk_pd_system_numba(t, y, dose_rate, params, bw, comed)\n",
        "\n",
        "    def get_cache_key(self, params, bw, comed, doses, dose_times):\n",
        "        \"\"\"\n",
        "        Create cache key for similar simulations\n",
        "        \"\"\"\n",
        "        # Round parameters to create cache keys\n",
        "        key_params = tuple(np.round(params, 3))\n",
        "        key_bw = round(bw, 1)\n",
        "        key_comed = int(comed)\n",
        "        key_doses = tuple(np.round(doses, 1))\n",
        "        key_dose_times = tuple(np.round(dose_times, 1))\n",
        "\n",
        "        return (key_params, key_bw, key_comed, key_doses, key_dose_times)\n",
        "\n",
        "    def simulate_individual_fast(self, params, times, doses, dose_times, bw, comed,\n",
        "                                baseline_R=8.0):\n",
        "        \"\"\"\n",
        "        High-performance individual simulation with multiple strategies\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Try neural ODE first\n",
        "        if self.neural_solver.available and self.neural_solver.trained:\n",
        "            conc, response = self.neural_solver.solve(\n",
        "                params, times, doses, dose_times, bw, comed, baseline_R\n",
        "            )\n",
        "            if conc is not None:\n",
        "                perf_monitor.log_metric('ode_solve_time', time.time() - start_time, increment=True)\n",
        "                return conc, response\n",
        "\n",
        "        # Check cache\n",
        "        cache_key = self.get_cache_key(params, bw, comed, doses, dose_times)\n",
        "        if cache_key in self.solution_cache:\n",
        "            cached_times, cached_conc, cached_response = self.solution_cache[cache_key]\n",
        "            if np.allclose(times, cached_times, rtol=1e-2):\n",
        "                # Interpolate cached solution\n",
        "                conc = np.interp(times, cached_times, cached_conc)\n",
        "                response = np.interp(times, cached_times, cached_response)\n",
        "                perf_monitor.log_metric('ode_solve_time', time.time() - start_time, increment=True)\n",
        "                return conc, response\n",
        "\n",
        "        # Use optimized traditional solver\n",
        "        dose_rates = create_dose_schedule(times, dose_times, doses)\n",
        "        dose_interp = interp1d(times, dose_rates, kind='linear', bounds_error=False, fill_value=0)\n",
        "\n",
        "        def dose_func(t):\n",
        "            return dose_interp(t)\n",
        "\n",
        "        # Initial conditions\n",
        "        y0 = [0, 0, 0, baseline_R]\n",
        "\n",
        "        try:\n",
        "            # Use optimized solver settings\n",
        "            sol = solve_ivp(\n",
        "                lambda t, y: self.pk_pd_system(t, y, dose_func, params, bw, comed),\n",
        "                [times[0], times[-1]], y0, t_eval=times,\n",
        "                method='DOP853',  # Higher order method\n",
        "                rtol=1e-4, atol=1e-7,  # Relaxed tolerances for speed\n",
        "                max_step=0.5\n",
        "            )\n",
        "\n",
        "            if sol.success:\n",
        "                A1, A2, AE, R = sol.y\n",
        "\n",
        "                # Calculate concentrations\n",
        "                V1BW = params[11]\n",
        "                V1_i = params[1] * (bw/70)**V1BW\n",
        "                concentrations = A1 / V1_i\n",
        "\n",
        "                # Cache successful solutions\n",
        "                if len(self.solution_cache) < 10000:  # Limit cache size\n",
        "                    self.solution_cache[cache_key] = (times.copy(), concentrations.copy(), R.copy())\n",
        "\n",
        "                perf_monitor.log_metric('ode_solve_time', time.time() - start_time, increment=True)\n",
        "                perf_monitor.log_metric('successful_subjects', 1, increment=True)\n",
        "                return concentrations, R\n",
        "            else:\n",
        "                perf_monitor.log_metric('failed_subjects', 1, increment=True)\n",
        "                return np.full_like(times, np.nan), np.full_like(times, np.nan)\n",
        "\n",
        "        except Exception as e:\n",
        "            perf_monitor.log_metric('failed_subjects', 1, increment=True)\n",
        "            perf_monitor.log_metric('ode_solve_time', time.time() - start_time, increment=True)\n",
        "            return np.full_like(times, np.nan), np.full_like(times, np.nan)\n",
        "\n",
        "    # Keep original method for compatibility\n",
        "    def simulate_individual(self, params, times, doses, dose_times, bw, comed, baseline_R=8.0):\n",
        "        return self.simulate_individual_fast(params, times, doses, dose_times, bw, comed, baseline_R)\n",
        "\n",
        "class SAEM_Estimator:\n",
        "    \"\"\"\n",
        "    Stochastic Approximation EM (SAEM) algorithm - faster than FOCE\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.data = None\n",
        "        self.theta = None\n",
        "        self.omega = None\n",
        "        self.sigma = None\n",
        "\n",
        "        # SAEM specific parameters\n",
        "        self.n_burn_in = 4\n",
        "        self.n_iterations = 20\n",
        "        self.n_chains = 4\n",
        "        self.step_size = 1.0\n",
        "\n",
        "        # Gaussian Process for parameter exploration\n",
        "        self.gp_surrogate = None\n",
        "\n",
        "    def load_data(self, df):\n",
        "        \"\"\"Load and prepare dataset with preprocessing\"\"\"\n",
        "        self.data = df.copy()\n",
        "\n",
        "        # Separate PK and PD data\n",
        "        self.pk_data = df[(df['DVID'] == 1) & (df['EVID'] == 0) & (df['MDV'] == 0)].copy()\n",
        "        self.pd_data = df[(df['DVID'] == 2) & (df['EVID'] == 0) & (df['MDV'] == 0)].copy()\n",
        "\n",
        "        # Preprocess for faster access\n",
        "        self.unique_subjects = df['ID'].unique()\n",
        "        self.subject_data_dict = {}\n",
        "\n",
        "        for subject_id in self.unique_subjects:\n",
        "            self.subject_data_dict[subject_id] = df[df['ID'] == subject_id].copy()\n",
        "\n",
        "        print(f\"Loaded {len(self.pk_data)} PK observations and {len(self.pd_data)} PD observations\")\n",
        "        print(f\"Preprocessed {len(self.unique_subjects)} subjects\")\n",
        "\n",
        "    def individual_likelihood_fast(self, eta_i, theta, omega, sigma, subject_data):\n",
        "        \"\"\"\n",
        "        Optimized individual likelihood calculation\n",
        "        \"\"\"\n",
        "        # Apply random effects\n",
        "        params = theta.copy()\n",
        "\n",
        "        # Vectorized application of random effects\n",
        "        pk_pd_indices = list(range(10))  # First 10 parameters\n",
        "        for i, idx in enumerate(pk_pd_indices):\n",
        "            if i < len(eta_i):\n",
        "                params[idx] *= np.exp(eta_i[i])\n",
        "\n",
        "        # Extract subject info\n",
        "        bw = subject_data['BW'].iloc[0]\n",
        "        comed = subject_data['COMED'].iloc[0]\n",
        "\n",
        "        # Separate observations\n",
        "        pk_obs = subject_data[subject_data['DVID'] == 1]\n",
        "        pd_obs = subject_data[subject_data['DVID'] == 2]\n",
        "\n",
        "        if len(pk_obs) == 0 and len(pd_obs) == 0:\n",
        "            return -np.inf\n",
        "\n",
        "        # Get dosing info\n",
        "        doses = subject_data[subject_data['EVID'] == 1]['AMT'].values\n",
        "        dose_times = subject_data[subject_data['EVID'] == 1]['TIME'].values\n",
        "\n",
        "        # Create optimized time vector\n",
        "        all_times = np.sort(subject_data['TIME'].unique())\n",
        "\n",
        "        try:\n",
        "            # Use fast simulation\n",
        "            conc_pred, response_pred = self.model.simulate_individual_fast(\n",
        "                params, all_times, doses, dose_times, bw, comed\n",
        "            )\n",
        "\n",
        "            log_likelihood = 0\n",
        "\n",
        "            # Vectorized PK likelihood\n",
        "            if len(pk_obs) > 0:\n",
        "                pk_times = pk_obs['TIME'].values\n",
        "                pk_observed = pk_obs['DV'].values\n",
        "\n",
        "                pk_predicted = np.interp(pk_times, all_times, conc_pred)\n",
        "                valid_idx = (pk_predicted > 0) & (pk_observed > 0) & np.isfinite(pk_predicted)\n",
        "\n",
        "                if np.sum(valid_idx) > 0:\n",
        "                    pk_residuals = np.log(pk_observed[valid_idx]) - np.log(pk_predicted[valid_idx])\n",
        "                    log_likelihood += -0.5 * np.sum(pk_residuals**2 / sigma[0]**2)\n",
        "                    log_likelihood += -0.5 * len(pk_residuals) * np.log(2 * np.pi * sigma[0]**2)\n",
        "                    log_likelihood += -np.sum(np.log(pk_observed[valid_idx]))\n",
        "\n",
        "            # Vectorized PD likelihood\n",
        "            if len(pd_obs) > 0:\n",
        "                pd_times = pd_obs['TIME'].values\n",
        "                pd_observed = pd_obs['DV'].values\n",
        "\n",
        "                pd_predicted = np.interp(pd_times, all_times, response_pred)\n",
        "                valid_idx = (pd_predicted > 0) & (pd_observed > 0) & np.isfinite(pd_predicted)\n",
        "\n",
        "                if np.sum(valid_idx) > 0:\n",
        "                    pd_residuals = pd_observed[valid_idx] - pd_predicted[valid_idx]\n",
        "                    pd_variance = (sigma[1] * pd_predicted[valid_idx])**2\n",
        "                    log_likelihood += -0.5 * np.sum(pd_residuals**2 / pd_variance)\n",
        "                    log_likelihood += -0.5 * np.sum(np.log(2 * np.pi * pd_variance))\n",
        "\n",
        "            # Prior for random effects\n",
        "            if len(eta_i) > 0:\n",
        "                log_likelihood += -0.5 * eta_i.T @ np.linalg.solve(omega, eta_i)\n",
        "\n",
        "        except Exception as e:\n",
        "            return -np.inf\n",
        "\n",
        "        return log_likelihood\n",
        "\n",
        "    def mcmc_step(self, eta, theta, omega, sigma, subject_data, step_size):\n",
        "        \"\"\"\n",
        "        Metropolis-Hastings step for SAEM\n",
        "        \"\"\"\n",
        "        # Propose new eta\n",
        "        eta_prop = eta + np.random.normal(0, step_size, len(eta))\n",
        "\n",
        "        # Calculate acceptance probability\n",
        "        ll_current = self.individual_likelihood_fast(eta, theta, omega, sigma, subject_data)\n",
        "        ll_prop = self.individual_likelihood_fast(eta_prop, theta, omega, sigma, subject_data)\n",
        "\n",
        "        alpha = min(1, np.exp(ll_prop - ll_current))\n",
        "\n",
        "        if np.random.random() < alpha:\n",
        "            return eta_prop, ll_prop\n",
        "        else:\n",
        "            return eta, ll_current\n",
        "\n",
        "    def saem_iteration(self, theta, omega, sigma):\n",
        "        \"\"\"\n",
        "        Single SAEM iteration with parallel processing\n",
        "        \"\"\"\n",
        "        def process_subject(subject_id):\n",
        "            subject_data = self.subject_data_dict[subject_id]\n",
        "\n",
        "            # Initialize or get previous eta\n",
        "            eta = np.random.multivariate_normal(np.zeros(len(omega)), omega * 0.1)\n",
        "\n",
        "            # MCMC steps for this subject\n",
        "            for _ in range(3):  # Few MCMC steps per SAEM iteration\n",
        "                eta, _ = self.mcmc_step(eta, theta, omega, sigma, subject_data, self.step_size)\n",
        "\n",
        "            return subject_id, eta\n",
        "\n",
        "        # Parallel processing of subjects\n",
        "        with ThreadPoolExecutor(max_workers=min(mp.cpu_count(), len(self.unique_subjects))) as executor:\n",
        "            results = list(executor.map(process_subject, self.unique_subjects))\n",
        "\n",
        "        # Collect eta estimates\n",
        "        eta_estimates = {}\n",
        "        for subject_id, eta in results:\n",
        "            eta_estimates[subject_id] = eta\n",
        "\n",
        "        return eta_estimates\n",
        "\n",
        "    def update_parameters(self, eta_estimates, iteration):\n",
        "        \"\"\"\n",
        "        Update population parameters using stochastic approximation\n",
        "        \"\"\"\n",
        "        # Learning rate schedule\n",
        "        gamma = min(1.0, 10.0 / (iteration + 10))\n",
        "\n",
        "        # Collect sufficient statistics\n",
        "        eta_values = np.array(list(eta_estimates.values()))\n",
        "\n",
        "        if len(eta_values) > 0:\n",
        "            # Update omega (between-subject variability)\n",
        "            empirical_cov = np.cov(eta_values.T)\n",
        "            self.omega = (1 - gamma) * self.omega + gamma * empirical_cov\n",
        "\n",
        "            # Ensure positive definiteness\n",
        "            eigenvals, eigenvecs = np.linalg.eigh(self.omega)\n",
        "            eigenvals = np.maximum(eigenvals, 1e-6)\n",
        "            self.omega = eigenvecs @ np.diag(eigenvals) @ eigenvecs.T\n",
        "\n",
        "    def fit(self, initial_params=None):\n",
        "        \"\"\"\n",
        "        Fit using SAEM algorithm\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        if initial_params is None:\n",
        "            # Smart initial parameter estimates\n",
        "            initial_params = np.array([\n",
        "                2.0, 10.0, 1.0, 20.0, 0.5,  # PK\n",
        "                0.1, 0.8, 2.0, 5.0, 0.1,    # PD\n",
        "                0.75, 1.0, 0.1, 0.1          # Covariates\n",
        "            ])\n",
        "\n",
        "        self.theta = initial_params.copy()\n",
        "        self.omega = np.eye(10) * 0.1  # Initial IIV\n",
        "        self.sigma = np.array([0.2, 0.15])  # Initial residual error\n",
        "\n",
        "        print(\"Starting SAEM parameter estimation...\")\n",
        "        print(f\"Burn-in: {self.n_burn_in} iterations\")\n",
        "        print(f\"Estimation: {self.n_iterations - self.n_burn_in} iterations\")\n",
        "\n",
        "        best_ll = -np.inf\n",
        "        best_params = self.theta.copy()\n",
        "\n",
        "        # SAEM iterations with progress bar\n",
        "        for iteration in tqdm(range(self.n_iterations), desc=\"SAEM Progress\"):\n",
        "            # E-step: Sample individual parameters\n",
        "            eta_estimates = self.saem_iteration(self.theta, self.omega, self.sigma)\n",
        "\n",
        "            # M-step: Update population parameters\n",
        "            if iteration > self.n_burn_in:\n",
        "                self.update_parameters(eta_estimates, iteration - self.n_burn_in)\n",
        "\n",
        "            # Monitor convergence every 50 iterations\n",
        "            if iteration % 50 == 0:\n",
        "                # Calculate approximate likelihood for monitoring\n",
        "                total_ll = 0\n",
        "                n_successful = 0\n",
        "\n",
        "                for subject_id in list(self.unique_subjects)[:min(50, len(self.unique_subjects))]:\n",
        "                    try:\n",
        "                        eta = eta_estimates.get(subject_id, np.zeros(len(self.omega)))\n",
        "                        ll = self.individual_likelihood_fast(\n",
        "                            eta, self.theta, self.omega, self.sigma,\n",
        "                            self.subject_data_dict[subject_id]\n",
        "                        )\n",
        "                        if np.isfinite(ll):\n",
        "                            total_ll += ll\n",
        "                            n_successful += 1\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "                if n_successful > 0:\n",
        "                    avg_ll = total_ll / n_successful\n",
        "                    if avg_ll > best_ll:\n",
        "                        best_ll = avg_ll\n",
        "                        best_params = self.theta.copy()\n",
        "\n",
        "                    print(f\"Iteration {iteration}: Avg LL = {avg_ll:.2f}, Best = {best_ll:.2f}\")\n",
        "\n",
        "            perf_monitor.log_metric('convergence_iterations', 1, increment=True)\n",
        "\n",
        "        # Final parameter estimates\n",
        "        self.theta = best_params\n",
        "\n",
        "        perf_monitor.log_metric('optimization_time', time.time() - start_time, increment=True)\n",
        "\n",
        "        print(f\"\\nSAEM converged after {self.n_iterations} iterations\")\n",
        "        print(\"Final parameter estimates:\")\n",
        "        for i, param_name in enumerate(self.model.all_params[:-2]):\n",
        "            if i < len(self.theta):\n",
        "                print(f\"  {param_name}: {self.theta[i]:.4f}\")\n",
        "\n",
        "        return self.theta, best_ll\n",
        "\n",
        "# Keep NLME_Estimator for backward compatibility but make it use SAEM\n",
        "class NLME_Estimator(SAEM_Estimator):\n",
        "    \"\"\"\n",
        "    NONMEM-style estimator now using faster SAEM algorithm\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "class PopulationSimulator:\n",
        "    \"\"\"\n",
        "    Enhanced Monte Carlo population simulation with GPU acceleration\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, theta, omega, sigma):\n",
        "        self.model = model\n",
        "        self.theta = theta\n",
        "        self.omega = omega\n",
        "        self.sigma = sigma\n",
        "\n",
        "        # Prepare for batch processing\n",
        "        self.batch_size = min(100, mp.cpu_count() * 2)\n",
        "\n",
        "    def generate_virtual_population_fast(self, n_subjects, bw_range=(50, 100),\n",
        "                                        comed_prob=0.5, seed=None):\n",
        "        \"\"\"\n",
        "        Vectorized virtual population generation\n",
        "        \"\"\"\n",
        "        if seed:\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        # Vectorized covariate generation\n",
        "        body_weights = np.random.uniform(bw_range[0], bw_range[1], n_subjects)\n",
        "        comed_status = np.random.binomial(1, comed_prob, n_subjects)\n",
        "\n",
        "        # Batch generate random effects\n",
        "        try:\n",
        "            eta_samples = np.random.multivariate_normal(\n",
        "                np.zeros(len(self.omega)), self.omega, size=n_subjects\n",
        "            )\n",
        "        except np.linalg.LinAlgError:\n",
        "            # Handle singular covariance\n",
        "            eta_samples = np.random.normal(0, 0.1, (n_subjects, len(self.omega)))\n",
        "\n",
        "        # Vectorized parameter calculation\n",
        "        virtual_population = []\n",
        "        theta_broadcast = np.broadcast_to(self.theta, (n_subjects, len(self.theta)))\n",
        "\n",
        "        for i in range(n_subjects):\n",
        "            individual_params = theta_broadcast[i].copy()\n",
        "\n",
        "            # Apply random effects\n",
        "            for j in range(min(eta_samples.shape[1], 10)):\n",
        "                if j < len(individual_params):\n",
        "                    individual_params[j] *= np.exp(eta_samples[i, j])\n",
        "\n",
        "            virtual_population.append({\n",
        "                'subject_id': i,\n",
        "                'bw': body_weights[i],\n",
        "                'comed': comed_status[i],\n",
        "                'params': individual_params,\n",
        "                'eta': eta_samples[i]\n",
        "            })\n",
        "\n",
        "        return virtual_population\n",
        "\n",
        "    def simulate_batch(self, batch_subjects, dose_mg, dosing_interval_h,\n",
        "                      simulation_days=28, steady_state_days=21):\n",
        "        \"\"\"\n",
        "        Simulate a batch of subjects in parallel\n",
        "        \"\"\"\n",
        "        def simulate_single(subject):\n",
        "            dt = 0.5\n",
        "            total_hours = simulation_days * 24\n",
        "            times = np.arange(0, total_hours + dt, dt)\n",
        "\n",
        "            dose_times = np.arange(0, total_hours, dosing_interval_h)\n",
        "            doses = np.full(len(dose_times), dose_mg)\n",
        "\n",
        "            try:\n",
        "                conc, response = self.model.simulate_individual_fast(\n",
        "                    subject['params'], times, doses, dose_times,\n",
        "                    subject['bw'], subject['comed']\n",
        "                )\n",
        "\n",
        "                if not (np.isnan(conc).all() or np.isnan(response).all()):\n",
        "                    steady_start_idx = int(steady_state_days * 24 / dt)\n",
        "                    steady_response = response[steady_start_idx:]\n",
        "                    steady_conc = conc[steady_start_idx:]\n",
        "\n",
        "                    # Target achievement logic\n",
        "                    if dosing_interval_h == 24:\n",
        "                        interval_size = int(24/dt)\n",
        "                    else:\n",
        "                        interval_size = int(168/dt)\n",
        "\n",
        "                    target_achieved = True\n",
        "                    for start in range(0, len(steady_response), interval_size):\n",
        "                        end = min(start + interval_size, len(steady_response))\n",
        "                        interval_response = steady_response[start:end]\n",
        "                        if len(interval_response) > 0 and not np.all(interval_response < 3.3):\n",
        "                            target_achieved = False\n",
        "                            break\n",
        "\n",
        "                    return {\n",
        "                        'subject_id': subject['subject_id'],\n",
        "                        'bw': subject['bw'],\n",
        "                        'comed': subject['comed'],\n",
        "                        'target_achieved': target_achieved,\n",
        "                        'min_response': np.min(steady_response),\n",
        "                        'mean_response': np.mean(steady_response),\n",
        "                        'max_conc': np.max(steady_conc)\n",
        "                    }\n",
        "                else:\n",
        "                    return None\n",
        "            except:\n",
        "                return None\n",
        "\n",
        "        # Use ThreadPoolExecutor for I/O bound simulation\n",
        "        with ThreadPoolExecutor(max_workers=min(len(batch_subjects), mp.cpu_count())) as executor:\n",
        "            results = list(executor.map(simulate_single, batch_subjects))\n",
        "\n",
        "        return [r for r in results if r is not None]\n",
        "\n",
        "    def simulate_dosing_regimen(self, virtual_population, dose_mg, dosing_interval_h,\n",
        "                               simulation_days=28, steady_state_days=21):\n",
        "        \"\"\"\n",
        "        High-performance dosing regimen simulation with batching\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        print(f\"Simulating {dose_mg} mg every {dosing_interval_h}h for {len(virtual_population)} subjects...\")\n",
        "        print(f\"Using batch processing with {self.batch_size} subjects per batch\")\n",
        "\n",
        "        all_results = []\n",
        "\n",
        "        # Process in batches with progress bar\n",
        "        for i in tqdm(range(0, len(virtual_population), self.batch_size), desc=\"Batch Progress\"):\n",
        "            batch_end = min(i + self.batch_size, len(virtual_population))\n",
        "            batch_subjects = virtual_population[i:batch_end]\n",
        "\n",
        "            batch_results = self.simulate_batch(\n",
        "                batch_subjects, dose_mg, dosing_interval_h,\n",
        "                simulation_days, steady_state_days\n",
        "            )\n",
        "            all_results.extend(batch_results)\n",
        "\n",
        "        perf_monitor.log_metric('simulation_time', time.time() - start_time, increment=True)\n",
        "        perf_monitor.log_metric('successful_subjects', len(all_results), increment=True)\n",
        "        perf_monitor.log_metric('failed_subjects', len(virtual_population) - len(all_results), increment=True)\n",
        "\n",
        "        print(f\"Successfully simulated {len(all_results)}/{len(virtual_population)} subjects\")\n",
        "\n",
        "        return pd.DataFrame(all_results)\n",
        "\n",
        "    def find_optimal_dose_adaptive(self, target_achievement=0.9, dose_range=(0.5, 20),\n",
        "                                  dosing_interval=24, n_subjects=5000, **population_kwargs):\n",
        "        \"\"\"\n",
        "        Adaptive dose finding with Gaussian Process surrogate\n",
        "        \"\"\"\n",
        "        print(f\"\\nAdaptive dose optimization for {target_achievement*100}% target achievement\")\n",
        "        print(f\"Dosing interval: {dosing_interval} hours\")\n",
        "\n",
        "        # Generate virtual population once\n",
        "        virtual_pop = self.generate_virtual_population_fast(n_subjects, **population_kwargs)\n",
        "\n",
        "        # Initialize Gaussian Process surrogate\n",
        "        kernel = Matern(length_scale=1.0, nu=2.5) + WhiteKernel(noise_level=0.01)\n",
        "        gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=2)\n",
        "\n",
        "        # Initial evaluations\n",
        "        initial_doses = np.linspace(dose_range[0], dose_range[1], 5)\n",
        "        dose_history = []\n",
        "        achievement_history = []\n",
        "\n",
        "        for dose in initial_doses:\n",
        "            if dosing_interval == 24:\n",
        "                test_dose = dose\n",
        "            else:\n",
        "                test_dose = dose * 7\n",
        "\n",
        "            results = self.simulate_dosing_regimen(virtual_pop, test_dose, dosing_interval)\n",
        "\n",
        "            if len(results) > 0:\n",
        "                achievement = results['target_achieved'].mean()\n",
        "                dose_history.append(dose)\n",
        "                achievement_history.append(achievement)\n",
        "                print(f\"  Dose {dose:.1f} mg: {achievement:.1%} achievement\")\n",
        "\n",
        "        # Fit initial GP\n",
        "        if len(dose_history) >= 3:\n",
        "            X = np.array(dose_history).reshape(-1, 1)\n",
        "            y = np.array(achievement_history)\n",
        "            gp.fit(X, y)\n",
        "\n",
        "        # Adaptive optimization\n",
        "        for iteration in range(10):  # Maximum 10 adaptive iterations\n",
        "            if len(dose_history) < 3:\n",
        "                break\n",
        "\n",
        "            # Acquisition function: Upper Confidence Bound\n",
        "            test_doses = np.linspace(dose_range[0], dose_range[1], 100)\n",
        "            mean_pred, std_pred = gp.predict(test_doses.reshape(-1, 1), return_std=True)\n",
        "\n",
        "            # UCB acquisition\n",
        "            beta = 2.0  # Exploration parameter\n",
        "            acquisition = mean_pred + beta * std_pred\n",
        "\n",
        "            # Find dose that maximizes acquisition near target\n",
        "            target_diff = np.abs(mean_pred - target_achievement)\n",
        "            acquisition_adjusted = acquisition - target_diff\n",
        "\n",
        "            next_dose_idx = np.argmax(acquisition_adjusted)\n",
        "            next_dose = test_doses[next_dose_idx]\n",
        "\n",
        "            # Skip if too close to existing evaluations\n",
        "            if min(np.abs(np.array(dose_history) - next_dose)) < 0.2:\n",
        "                break\n",
        "\n",
        "            # Evaluate new dose\n",
        "            if dosing_interval == 24:\n",
        "                test_dose = next_dose\n",
        "            else:\n",
        "                test_dose = next_dose * 7\n",
        "\n",
        "            results = self.simulate_dosing_regimen(virtual_pop, test_dose, dosing_interval)\n",
        "\n",
        "            if len(results) > 0:\n",
        "                achievement = results['target_achieved'].mean()\n",
        "                dose_history.append(next_dose)\n",
        "                achievement_history.append(achievement)\n",
        "\n",
        "                print(f\"  Adaptive iteration {iteration+1}: Dose {next_dose:.1f} mg: {achievement:.1%}\")\n",
        "\n",
        "                # Update GP\n",
        "                X = np.array(dose_history).reshape(-1, 1)\n",
        "                y = np.array(achievement_history)\n",
        "                gp.fit(X, y)\n",
        "\n",
        "                # Check convergence\n",
        "                if abs(achievement - target_achievement) < 0.02:\n",
        "                    print(f\"  Converged at dose {next_dose:.1f} mg\")\n",
        "                    return round(next_dose * 2) / 2 if dosing_interval == 24 else round(next_dose / 5) * 5\n",
        "\n",
        "        # Return best dose from evaluations\n",
        "        achievement_array = np.array(achievement_history)\n",
        "        target_mask = achievement_array >= target_achievement\n",
        "\n",
        "        if np.any(target_mask):\n",
        "            valid_doses = np.array(dose_history)[target_mask]\n",
        "            optimal_dose = np.min(valid_doses)  # Minimum effective dose\n",
        "        else:\n",
        "            # Return dose with highest achievement\n",
        "            optimal_dose = dose_history[np.argmax(achievement_array)]\n",
        "\n",
        "        # Round appropriately\n",
        "        if dosing_interval == 24:\n",
        "            optimal_dose = round(optimal_dose * 2) / 2\n",
        "        else:\n",
        "            optimal_dose = round(optimal_dose / 5) * 5\n",
        "\n",
        "        return optimal_dose\n",
        "\n",
        "    # Keep original method for backward compatibility\n",
        "    def find_optimal_dose(self, target_achievement=0.9, dose_range=(0.5, 20),\n",
        "                         dosing_interval=24, n_subjects=5000, **population_kwargs):\n",
        "        return self.find_optimal_dose_adaptive(target_achievement, dose_range,\n",
        "                                              dosing_interval, n_subjects, **population_kwargs)\n",
        "\n",
        "    def generate_virtual_population(self, n_subjects, **kwargs):\n",
        "        \"\"\"Backward compatibility method\"\"\"\n",
        "        return self.generate_virtual_population_fast(n_subjects, **kwargs)\n",
        "\n",
        "def run_complete_simulation_enhanced(df):\n",
        "    \"\"\"\n",
        "    Enhanced complete simulation with performance monitoring\n",
        "    \"\"\"\n",
        "    perf_monitor.reset()\n",
        "\n",
        "    print(\"STARTING ENHANCED PKPD SIMULATION\")\n",
        "    print(f\"CPU Cores Available: {mp.cpu_count()}\")\n",
        "    print(f\"GPU Available: {USE_GPU}\")\n",
        "    if TORCH_AVAILABLE:\n",
        "        print(\"Neural ODE Support: Available\")\n",
        "    else:\n",
        "        print(\"Neural ODE Support: Not available\")\n",
        "\n",
        "\n",
        "    # Initialize enhanced model and estimator\n",
        "    model = PK_PD_Model()\n",
        "    estimator = SAEM_Estimator(model)  # Using SAEM instead of FOCE\n",
        "    estimator.load_data(df)\n",
        "\n",
        "    # Train Neural ODE if available\n",
        "    if model.neural_solver.available:\n",
        "        print(\"\\nTraining Neural ODE surrogate...\")\n",
        "        model.neural_solver.train_on_traditional_solver()\n",
        "\n",
        "    # Fit model using SAEM\n",
        "    print(\"\\nFITTING NLME MODEL WITH SAEM...\")\n",
        "    final_params, final_ll = estimator.fit()\n",
        "\n",
        "    # Initialize enhanced simulator\n",
        "    simulator = PopulationSimulator(model, estimator.theta, estimator.omega, estimator.sigma)\n",
        "\n",
        "    # Enhanced dose optimization\n",
        "\n",
        "    print(\"ENHANCED DOSE OPTIMIZATION RESULTS\")\n",
        "\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Use adaptive optimization for all scenarios\n",
        "    print(\"\\n1. Daily dose for 90% target achievement (original population):\")\n",
        "    daily_dose_90 = simulator.find_optimal_dose_adaptive(\n",
        "        target_achievement=0.9, dosing_interval=24,\n",
        "        bw_range=(50, 100), comed_prob=0.5, n_subjects=3000\n",
        "    )\n",
        "    results['daily_90_original'] = daily_dose_90\n",
        "    print(f\"   Optimal daily dose: {daily_dose_90:.1f} mg\")\n",
        "\n",
        "    print(\"\\n2. Weekly dose for 90% target achievement (original population):\")\n",
        "    weekly_dose_90 = simulator.find_optimal_dose_adaptive(\n",
        "        target_achievement=0.9, dosing_interval=168,\n",
        "        bw_range=(50, 100), comed_prob=0.5, n_subjects=3000\n",
        "    )\n",
        "    results['weekly_90_original'] = weekly_dose_90\n",
        "    print(f\"   Optimal weekly dose: {weekly_dose_90:.0f} mg\")\n",
        "\n",
        "    print(\"\\n3. Effect of changed body weight distribution (70-140 kg):\")\n",
        "    daily_dose_90_heavy = simulator.find_optimal_dose_adaptive(\n",
        "        target_achievement=0.9, dosing_interval=24,\n",
        "        bw_range=(70, 140), comed_prob=0.5, n_subjects=3000\n",
        "    )\n",
        "    weekly_dose_90_heavy = simulator.find_optimal_dose_adaptive(\n",
        "        target_achievement=0.9, dosing_interval=168,\n",
        "        bw_range=(70, 140), comed_prob=0.5, n_subjects=3000\n",
        "    )\n",
        "    results['daily_90_heavy'] = daily_dose_90_heavy\n",
        "    results['weekly_90_heavy'] = weekly_dose_90_heavy\n",
        "    print(f\"   Daily dose (heavy population): {daily_dose_90_heavy:.1f} mg\")\n",
        "    print(f\"   Weekly dose (heavy population): {weekly_dose_90_heavy:.0f} mg\")\n",
        "\n",
        "    print(\"\\n4. Effect of restricting concomitant medication:\")\n",
        "    daily_dose_90_no_comed = simulator.find_optimal_dose_adaptive(\n",
        "        target_achievement=0.9, dosing_interval=24,\n",
        "        bw_range=(50, 100), comed_prob=0.0, n_subjects=3000\n",
        "    )\n",
        "    weekly_dose_90_no_comed = simulator.find_optimal_dose_adaptive(\n",
        "        target_achievement=0.9, dosing_interval=168,\n",
        "        bw_range=(50, 100), comed_prob=0.0, n_subjects=3000\n",
        "    )\n",
        "    results['daily_90_no_comed'] = daily_dose_90_no_comed\n",
        "    results['weekly_90_no_comed'] = weekly_dose_90_no_comed\n",
        "    print(f\"   Daily dose (no COMED): {daily_dose_90_no_comed:.1f} mg\")\n",
        "    print(f\"   Weekly dose (no COMED): {weekly_dose_90_no_comed:.0f} mg\")\n",
        "\n",
        "    print(\"\\n5. Doses for 75% target achievement:\")\n",
        "\n",
        "    # 75% scenarios with parallel execution\n",
        "    scenarios_75 = [\n",
        "        ('original', (50, 100), 0.5),\n",
        "        ('heavy', (70, 140), 0.5),\n",
        "        ('no_comed', (50, 100), 0.0)\n",
        "    ]\n",
        "\n",
        "    for scenario_name, bw_range, comed_prob in scenarios_75:\n",
        "        daily_dose_75 = simulator.find_optimal_dose_adaptive(\n",
        "            target_achievement=0.75, dosing_interval=24,\n",
        "            bw_range=bw_range, comed_prob=comed_prob, n_subjects=2000\n",
        "        )\n",
        "        weekly_dose_75 = simulator.find_optimal_dose_adaptive(\n",
        "            target_achievement=0.75, dosing_interval=168,\n",
        "            bw_range=bw_range, comed_prob=comed_prob, n_subjects=2000\n",
        "        )\n",
        "\n",
        "        results[f'daily_75_{scenario_name}'] = daily_dose_75\n",
        "        results[f'weekly_75_{scenario_name}'] = weekly_dose_75\n",
        "\n",
        "        print(f\"   {scenario_name.replace('_', ' ').title()} population:\")\n",
        "        print(f\"     Daily (75%): {daily_dose_75:.1f} mg\")\n",
        "        print(f\"     Weekly (75%): {weekly_dose_75:.0f} mg\")\n",
        "\n",
        "    # Enhanced summary with performance metrics\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ENHANCED SUMMARY OF OPTIMAL DOSES\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    summary_data = []\n",
        "    scenarios = ['original', 'heavy', 'no_comed']\n",
        "    scenario_names = ['Original Pop', 'Heavy Pop (70-140kg)', 'No COMED']\n",
        "\n",
        "    for scenario, name in zip(scenarios, scenario_names):\n",
        "        summary_data.extend([\n",
        "            {\n",
        "                'Scenario': f'{name} (90%)',\n",
        "                'Daily (mg)': results[f'daily_90_{scenario}'],\n",
        "                'Weekly (mg)': results[f'weekly_90_{scenario}'],\n",
        "                'Daily vs Weekly Ratio': results[f'weekly_90_{scenario}'] / (results[f'daily_90_{scenario}'] * 7)\n",
        "            },\n",
        "            {\n",
        "                'Scenario': f'{name} (75%)',\n",
        "                'Daily (mg)': results[f'daily_75_{scenario}'],\n",
        "                'Weekly (mg)': results[f'weekly_75_{scenario}'],\n",
        "                'Daily vs Weekly Ratio': results[f'weekly_75_{scenario}'] / (results[f'daily_75_{scenario}'] * 7)\n",
        "            }\n",
        "        ])\n",
        "\n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    print(summary_df.to_string(index=False, float_format='%.2f'))\n",
        "\n",
        "    # Enhanced dose reduction analysis\n",
        "    print(\"\\nENHANCED DOSE REDUCTION ANALYSIS (75% vs 90% achievement):\")\n",
        "    for scenario, name in zip(scenarios, scenario_names):\n",
        "        daily_90_key = f'daily_90_{scenario}'\n",
        "        daily_75_key = f'daily_75_{scenario}'\n",
        "        weekly_90_key = f'weekly_90_{scenario}'\n",
        "        weekly_75_key = f'weekly_75_{scenario}'\n",
        "\n",
        "        daily_reduction = results[daily_90_key] - results[daily_75_key]\n",
        "        weekly_reduction = results[weekly_90_key] - results[weekly_75_key]\n",
        "\n",
        "        daily_pct_reduction = (daily_reduction / results[daily_90_key]) * 100\n",
        "        weekly_pct_reduction = (weekly_reduction / results[weekly_90_key]) * 100\n",
        "\n",
        "        print(f\"   {name}:\")\n",
        "        print(f\"     Daily: -{daily_reduction:.1f} mg ({daily_pct_reduction:.1f}% reduction)\")\n",
        "        print(f\"     Weekly: -{weekly_reduction:.0f} mg ({weekly_pct_reduction:.1f}% reduction)\")\n",
        "        print(f\"     Efficiency gain: {weekly_pct_reduction - daily_pct_reduction:.1f}% better with weekly dosing\")\n",
        "\n",
        "    # Performance report\n",
        "    perf_monitor.report()\n",
        "\n",
        "    return results, estimator, simulator\n",
        "\n",
        "def create_enhanced_diagnostic_plots(estimator, simulator, results):\n",
        "    \"\"\"\n",
        "    Create enhanced diagnostic plots with performance insights\n",
        "    \"\"\"\n",
        "    print(\"\\nGENERATING ENHANCED DIAGNOSTIC PLOTS...\")\n",
        "\n",
        "    fig, axes = plt.subplots(3, 3, figsize=(20, 16))\n",
        "\n",
        "    # Generate comprehensive simulation data\n",
        "    virtual_pop = simulator.generate_virtual_population_fast(1500, seed=42)\n",
        "    sim_results_daily = simulator.simulate_dosing_regimen(virtual_pop, 5.0, 24)\n",
        "    sim_results_weekly = simulator.simulate_dosing_regimen(virtual_pop, 35.0, 168)\n",
        "\n",
        "    # Plot 1: Enhanced Parameter Estimates with Confidence Intervals\n",
        "    ax1 = axes[0, 0]\n",
        "    param_names = ['CL', 'V1', 'Q', 'V2', 'KA', 'KE0', 'IMAX', 'IC50', 'KIN', 'KOUT']\n",
        "    param_values = estimator.theta[:10]\n",
        "    param_se = np.sqrt(np.diag(estimator.omega))[:10] * 0.1  # Approximate SE\n",
        "\n",
        "    bars = ax1.bar(range(len(param_names)), param_values,\n",
        "                  yerr=param_se, capsize=5, color='skyblue', alpha=0.7)\n",
        "    ax1.set_xlabel('Parameters')\n",
        "    ax1.set_ylabel('Estimated Values ± SE')\n",
        "    ax1.set_title('Population Parameter Estimates with Uncertainty')\n",
        "    ax1.set_xticks(range(len(param_names)))\n",
        "    ax1.set_xticklabels(param_names, rotation=45)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: Inter-Individual Variability Heatmap\n",
        "    ax2 = axes[0, 1]\n",
        "    omega_matrix = estimator.omega[:len(param_names), :len(param_names)]\n",
        "    correlation_matrix = np.corrcoef(omega_matrix)\n",
        "\n",
        "    im = ax2.imshow(correlation_matrix, cmap='RdYlBu_r', vmin=-1, vmax=1)\n",
        "    ax2.set_xticks(range(len(param_names)))\n",
        "    ax2.set_yticks(range(len(param_names)))\n",
        "    ax2.set_xticklabels(param_names, rotation=45)\n",
        "    ax2.set_yticklabels(param_names)\n",
        "    ax2.set_title('Parameter Correlation Matrix')\n",
        "    plt.colorbar(im, ax=ax2)\n",
        "\n",
        "    # Plot 3: Target Achievement vs Body Weight (Both Dosing Regimens)\n",
        "    ax3 = axes[0, 2]\n",
        "    if len(sim_results_daily) > 0 and len(sim_results_weekly) > 0:\n",
        "        bw_bins = np.linspace(50, 100, 6)\n",
        "        daily_achievement = []\n",
        "        weekly_achievement = []\n",
        "\n",
        "        for i in range(len(bw_bins)-1):\n",
        "            mask_daily = (sim_results_daily['bw'] >= bw_bins[i]) & (sim_results_daily['bw'] < bw_bins[i+1])\n",
        "            mask_weekly = (sim_results_weekly['bw'] >= bw_bins[i]) & (sim_results_weekly['bw'] < bw_bins[i+1])\n",
        "\n",
        "            daily_achievement.append(sim_results_daily[mask_daily]['target_achieved'].mean() * 100)\n",
        "            weekly_achievement.append(sim_results_weekly[mask_weekly]['target_achieved'].mean() * 100)\n",
        "\n",
        "        bw_centers = (bw_bins[:-1] + bw_bins[1:]) / 2\n",
        "        ax3.plot(bw_centers, daily_achievement, 'o-', label='Daily', linewidth=2, markersize=6)\n",
        "        ax3.plot(bw_centers, weekly_achievement, 's-', label='Weekly', linewidth=2, markersize=6)\n",
        "        ax3.axhline(y=90, color='red', linestyle='--', alpha=0.7, label='90% Target')\n",
        "        ax3.set_xlabel('Body Weight (kg)')\n",
        "        ax3.set_ylabel('Achievement Rate (%)')\n",
        "        ax3.set_title('Target Achievement vs Body Weight')\n",
        "        ax3.grid(True, alpha=0.3)\n",
        "        ax3.legend()\n",
        "\n",
        "    # Plot 4: Advanced Dose-Response Surface\n",
        "    ax4 = axes[1, 0]\n",
        "    dose_range = np.linspace(1, 12, 8)\n",
        "    bw_range = np.linspace(50, 100, 6)\n",
        "    achievement_surface = np.zeros((len(bw_range), len(dose_range)))\n",
        "\n",
        "    for i, bw_center in enumerate(bw_range):\n",
        "        for j, dose in enumerate(dose_range):\n",
        "            test_pop = simulator.generate_virtual_population_fast(\n",
        "                200, bw_range=(bw_center-5, bw_center+5), seed=42+i*10+j\n",
        "            )\n",
        "            test_results = simulator.simulate_dosing_regimen(test_pop, dose, 24)\n",
        "            if len(test_results) > 0:\n",
        "                achievement_surface[i, j] = test_results['target_achieved'].mean() * 100\n",
        "\n",
        "    im4 = ax4.contourf(dose_range, bw_range, achievement_surface,\n",
        "                       levels=np.linspace(0, 100, 11), cmap='viridis')\n",
        "    ax4.contour(dose_range, bw_range, achievement_surface,\n",
        "                levels=[75, 90], colors=['orange', 'red'], linewidths=2)\n",
        "    ax4.set_xlabel('Daily Dose (mg)')\n",
        "    ax4.set_ylabel('Body Weight (kg)')\n",
        "    ax4.set_title('Achievement Rate Surface (%)')\n",
        "    plt.colorbar(im4, ax=ax4)\n",
        "\n",
        "    # Plot 5: COMED Effect with Statistical Significance\n",
        "    ax5 = axes[1, 1]\n",
        "    if len(sim_results_daily) > 0:\n",
        "        comed_groups = sim_results_daily.groupby('comed')['target_achieved']\n",
        "        comed_means = comed_groups.mean() * 100\n",
        "        comed_stds = comed_groups.std() * 100\n",
        "        comed_counts = comed_groups.count()\n",
        "        comed_se = comed_stds / np.sqrt(comed_counts)\n",
        "\n",
        "        comed_labels = ['No COMED', 'COMED']\n",
        "        bars5 = ax5.bar(comed_labels, comed_means.values,\n",
        "                       yerr=comed_se.values, capsize=5,\n",
        "                       color=['blue', 'orange'], alpha=0.7)\n",
        "        ax5.set_ylabel('Achievement Rate (%) ± SE')\n",
        "        ax5.set_title('Effect of Concomitant Medication')\n",
        "        ax5.axhline(y=90, color='red', linestyle='--', alpha=0.7)\n",
        "        ax5.grid(True, alpha=0.3)\n",
        "\n",
        "        # Add significance test result\n",
        "        from scipy.stats import ttest_ind\n",
        "        group0 = sim_results_daily[sim_results_daily['comed']==0]['target_achieved']\n",
        "        group1 = sim_results_daily[sim_results_daily['comed']==1]['target_achieved']\n",
        "        if len(group0) > 0 and len(group1) > 0:\n",
        "            t_stat, p_val = ttest_ind(group0, group1)\n",
        "            ax5.text(0.5, max(comed_means) + 5, f'p = {p_val:.3f}',\n",
        "                    ha='center', transform=ax5.transData)\n",
        "\n",
        "    # Plot 6: Performance Metrics Dashboard\n",
        "    ax6 = axes[1, 2]\n",
        "    metrics_names = ['Successful\\nSubjects', 'ODE\\nEvaluations', 'Cache\\nHits', 'Memory\\n(MB)']\n",
        "    metrics_values = [\n",
        "        perf_monitor.metrics['successful_subjects'],\n",
        "        perf_monitor.metrics['ode_evaluations'],\n",
        "        len(simulator.model.solution_cache),\n",
        "        perf_monitor.metrics['memory_peak_mb']\n",
        "    ]\n",
        "\n",
        "    # Normalize values for display\n",
        "    normalized_values = [v/max(metrics_values) * 100 for v in metrics_values]\n",
        "    bars6 = ax6.bar(metrics_names, normalized_values,\n",
        "                   color=['green', 'blue', 'purple', 'red'], alpha=0.7)\n",
        "    ax6.set_ylabel('Normalized Performance Score')\n",
        "    ax6.set_title('Performance Metrics Dashboard')\n",
        "\n",
        "    # Add actual values as text\n",
        "    for bar, actual in zip(bars6, metrics_values):\n",
        "        height = bar.get_height()\n",
        "        ax6.text(bar.get_x() + bar.get_width()/2, height + 2,\n",
        "                f'{actual:.0f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    # Plot 7: Residual Error Analysis\n",
        "    ax7 = axes[2, 0]\n",
        "    error_types = ['PK Error\\n(Log-normal)', 'PD Error\\n(Proportional)']\n",
        "    error_values = [estimator.sigma[0] * 100, estimator.sigma[1] * 100]\n",
        "    error_colors = ['purple', 'brown']\n",
        "\n",
        "    bars7 = ax7.bar(error_types, error_values, color=error_colors, alpha=0.7)\n",
        "    ax7.set_ylabel('Error Magnitude (%)')\n",
        "    ax7.set_title('Residual Error Model Parameters')\n",
        "    ax7.grid(True, alpha=0.3)\n",
        "\n",
        "    for bar, val in zip(bars7, error_values):\n",
        "        ax7.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
        "                f'{val:.1f}%', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "\n",
        "    # Plot 8: Dosing Efficiency Comparison\n",
        "    ax9 = axes[2, 2]\n",
        "    scenarios = ['Original', 'Heavy BW', 'No COMED']\n",
        "    daily_doses = [\n",
        "        results.get('daily_90_original', 0),\n",
        "        results.get('daily_90_heavy', 0),\n",
        "        results.get('daily_90_no_comed', 0)\n",
        "    ]\n",
        "    weekly_doses = [\n",
        "        results.get('weekly_90_original', 0) / 7,  # Convert to daily equivalent\n",
        "        results.get('weekly_90_heavy', 0) / 7,\n",
        "        results.get('weekly_90_no_comed', 0) / 7\n",
        "    ]\n",
        "\n",
        "    x = np.arange(len(scenarios))\n",
        "    width = 0.35\n",
        "\n",
        "    bars_daily = ax9.bar(x - width/2, daily_doses, width, label='Daily', alpha=0.7)\n",
        "    bars_weekly = ax9.bar(x + width/2, weekly_doses, width, label='Weekly (equiv.)', alpha=0.7)\n",
        "\n",
        "    ax9.set_xlabel('Population Scenario')\n",
        "    ax9.set_ylabel('Daily Dose Equivalent (mg)')\n",
        "    ax9.set_title('Dosing Efficiency: Daily vs Weekly')\n",
        "    ax9.set_xticks(x)\n",
        "    ax9.set_xticklabels(scenarios)\n",
        "    ax9.legend()\n",
        "    ax9.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add efficiency percentages\n",
        "    for i, (daily, weekly) in enumerate(zip(daily_doses, weekly_doses)):\n",
        "        if daily > 0:\n",
        "            efficiency = (daily - weekly) / daily * 100\n",
        "            ax9.text(i, max(daily, weekly) + 0.1, f'{efficiency:.1f}%\\nsavings',\n",
        "                    ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "results, estimator, simulator = run_complete_simulation_enhanced(df)\n",
        "create_enhanced_diagnostic_plots(estimator, simulator)"
      ],
      "metadata": {
        "id": "rs1elx9qUzw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantum Enhanced Implementation"
      ],
      "metadata": {
        "id": "Muis4GTpV4ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.integrate import solve_ivp\n",
        "from scipy.optimize import minimize, differential_evolution\n",
        "from scipy.stats import multivariate_normal, norm\n",
        "from scipy.interpolate import interp1d\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Quantum computing imports\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as qnp\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import jit, vmap\n",
        "from functools import partial\n",
        "\n",
        "# Performance imports\n",
        "from numba import jit as numba_jit\n",
        "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
        "import multiprocessing as mp\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import Matern, WhiteKernel\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import psutil\n",
        "\n",
        "# Set plotting style (same as original)\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "# Quantum device setup for compartment simulation\n",
        "n_qubits = 12  # 3 qubits per compartment for d=8 levels\n",
        "dev = qml.device('default.qubit', wires=n_qubits)\n",
        "\n",
        "print(\"Quantum-Enhanced PK/PD Framework Initialized\")\n",
        "print(f\"Quantum Device: {n_qubits} qubits for compartment encoding\")\n",
        "\n",
        "class PerformanceMonitor:\n",
        "    \"\"\"Monitor and report performance metrics (same as original)\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.start_time = time.time()\n",
        "        self.metrics = {\n",
        "            'total_time': 0,\n",
        "            'ode_solve_time': 0,\n",
        "            'optimization_time': 0,\n",
        "            'simulation_time': 0,\n",
        "            'memory_peak_mb': 0,\n",
        "            'cpu_count': mp.cpu_count(),\n",
        "            'successful_subjects': 0,\n",
        "            'failed_subjects': 0,\n",
        "            'ode_evaluations': 0,\n",
        "            'convergence_iterations': 0\n",
        "        }\n",
        "\n",
        "    def log_metric(self, key, value, increment=False):\n",
        "        if increment:\n",
        "            self.metrics[key] += value\n",
        "        else:\n",
        "            self.metrics[key] = value\n",
        "\n",
        "    def get_memory_usage(self):\n",
        "        return psutil.Process().memory_info().rss / 1024 / 1024\n",
        "\n",
        "    def report(self):\n",
        "        self.metrics['total_time'] = time.time() - self.start_time\n",
        "        self.metrics['memory_peak_mb'] = self.get_memory_usage()\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"PERFORMANCE REPORT\")\n",
        "        print(f\"Total Runtime: {self.metrics['total_time']:.2f} seconds\")\n",
        "        print(f\"ODE Solving: {self.metrics['ode_solve_time']:.2f} seconds\")\n",
        "        print(f\"Optimization: {self.metrics['optimization_time']:.2f} seconds\")\n",
        "        print(f\"Simulation: {self.metrics['simulation_time']:.2f} seconds\")\n",
        "        print(f\"Peak Memory: {self.metrics['memory_peak_mb']:.1f} MB\")\n",
        "        print(f\"CPU Cores Used: {self.metrics['cpu_count']}\")\n",
        "        print(f\"Successful Subjects: {self.metrics['successful_subjects']}\")\n",
        "        print(f\"Failed Subjects: {self.metrics['failed_subjects']}\")\n",
        "        print(f\"ODE Evaluations: {self.metrics['ode_evaluations']}\")\n",
        "        print(f\"Optimization Iterations: {self.metrics['convergence_iterations']}\")\n",
        "\n",
        "        if self.metrics['successful_subjects'] > 0:\n",
        "            success_rate = self.metrics['successful_subjects'] / (self.metrics['successful_subjects'] + self.metrics['failed_subjects']) * 100\n",
        "            print(f\"Success Rate: {success_rate:.1f}%\")\n",
        "\n",
        "# Global performance monitor (same as original)\n",
        "perf_monitor = PerformanceMonitor()\n",
        "\n",
        "class QuantumCompartmentSimulator:\n",
        "    \"\"\"\n",
        "    Quantum simulator for PK/PD compartments using Lindblad master equation\n",
        "    Based on the theoretical framework from your document\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.d_levels = 8  # Truncated bosonic levels per compartment\n",
        "        self.n_qubits_per_comp = 3  # log2(8) = 3 qubits per compartment\n",
        "        self.compartments = ['A1', 'A2', 'AE', 'R']  # 4 compartments\n",
        "        self.use_quantum = True\n",
        "\n",
        "    def create_lindblad_operators(self, params, bw, comed):\n",
        "        \"\"\"\n",
        "        Create Lindblad jump operators for PK/PD flows\n",
        "        Maps classical rates to quantum jump operators as per your document\n",
        "        \"\"\"\n",
        "        # Extract parameters (same as classical)\n",
        "        CL, V1, Q, V2, KA = params[0], params[1], params[2], params[3], params[4]\n",
        "        KE0, IMAX, IC50, KIN, KOUT = params[5], params[6], params[7], params[8], params[9]\n",
        "        CLBW, V1BW, CLCOMED, KINCOMED = params[10], params[11], params[12], params[13]\n",
        "\n",
        "        # Covariate effects (same as classical)\n",
        "        CL_i = CL * ((bw/70.0)**CLBW) * (1.0 + CLCOMED * comed)\n",
        "        V1_i = V1 * ((bw/70.0)**V1BW)\n",
        "        KIN_i = KIN * (1.0 + KINCOMED * comed)\n",
        "\n",
        "        # Calculate quantum jump rates (mapping from your document)\n",
        "        rates = {\n",
        "            'k12': Q / V1_i,      # A1 → A2 flow\n",
        "            'k21': Q / V2,        # A2 → A1 flow\n",
        "            'k_el_1': CL_i / V1_i, # A1 elimination\n",
        "            'KE0': KE0,           # A1 ↔ AE coupling\n",
        "            'KOUT': KOUT          # R elimination\n",
        "        }\n",
        "\n",
        "        return rates\n",
        "\n",
        "    def quantum_evolution_circuit(self, params, rates, dt, dose_rate):\n",
        "        \"\"\"\n",
        "        Quantum circuit for one time step of Lindblad evolution\n",
        "        Implements simplified version of the quantum dynamics from your framework\n",
        "        \"\"\"\n",
        "        @qml.qnode(dev)\n",
        "        def circuit():\n",
        "            # Initialize state (ground state for empty compartments)\n",
        "            qml.BasisState([0]*n_qubits, wires=range(n_qubits))\n",
        "\n",
        "            # Apply dosing to A1 compartment (qubits 0-2)\n",
        "            if dose_rate > 0:\n",
        "                for qubit in range(3):\n",
        "                    qml.RY(np.sqrt(dose_rate * dt) * 0.01, wires=qubit)\n",
        "\n",
        "            # Apply inter-compartment transfers (A1↔A2)\n",
        "            for i in range(3):\n",
        "                # A1 → A2 transfer\n",
        "                qml.CRY(np.sqrt(rates['k12'] * dt) * 0.01, wires=[i, i+3])\n",
        "                # A2 → A1 transfer\n",
        "                qml.CRY(np.sqrt(rates['k21'] * dt) * 0.01, wires=[i+3, i])\n",
        "\n",
        "            # Apply A1 ↔ AE coupling\n",
        "            for i in range(3):\n",
        "                qml.CRY(np.sqrt(rates['KE0'] * dt) * 0.01, wires=[i, i+6])\n",
        "                qml.CRY(np.sqrt(rates['KE0'] * dt) * 0.01, wires=[i+6, i])\n",
        "\n",
        "            # Apply elimination from A1 and R\n",
        "            for qubit in range(3):\n",
        "                qml.RY(-np.sqrt(rates['k_el_1'] * dt) * 0.01, wires=qubit)  # A1 elimination\n",
        "                qml.RY(-np.sqrt(rates['KOUT'] * dt) * 0.01, wires=qubit+9)  # R elimination\n",
        "\n",
        "            return qml.probs(wires=range(n_qubits))\n",
        "\n",
        "        return circuit()\n",
        "        # Initialize state (ground state for empty compartments)\n",
        "        qml.BasisState([0]*n_qubits, wires=range(n_qubits))\n",
        "\n",
        "        # Apply dosing to A1 compartment (qubits 0-2)\n",
        "        if dose_rate > 0:\n",
        "            for qubit in range(3):\n",
        "                qml.RY(np.sqrt(dose_rate * dt) * 0.01, wires=qubit)\n",
        "\n",
        "        # Apply inter-compartment transfers (A1↔A2)\n",
        "        for i in range(3):\n",
        "            # A1 → A2 transfer\n",
        "            qml.CRY(np.sqrt(rates['k12'] * dt) * 0.01, wires=[i, i+3])\n",
        "            # A2 → A1 transfer\n",
        "            qml.CRY(np.sqrt(rates['k21'] * dt) * 0.01, wires=[i+3, i])\n",
        "\n",
        "        # Apply A1 ↔ AE coupling\n",
        "        for i in range(3):\n",
        "            qml.CRY(np.sqrt(rates['KE0'] * dt) * 0.01, wires=[i, i+6])\n",
        "            qml.CRY(np.sqrt(rates['KE0'] * dt) * 0.01, wires=[i+6, i])\n",
        "\n",
        "        # Apply elimination from A1 and R\n",
        "        for qubit in range(3):\n",
        "            qml.RY(-np.sqrt(rates['k_el_1'] * dt) * 0.01, wires=qubit)  # A1 elimination\n",
        "            qml.RY(-np.sqrt(rates['KOUT'] * dt) * 0.01, wires=qubit+9)  # R elimination\n",
        "\n",
        "        return qml.probs(wires=range(n_qubits))\n",
        "\n",
        "    def measure_concentrations(self, quantum_probs):\n",
        "        \"\"\"\n",
        "        Extract concentration expectations from quantum probabilities\n",
        "        Maps quantum state to classical concentrations\n",
        "        \"\"\"\n",
        "        # Simplified measurement: extract expectation values for each compartment\n",
        "        # In practice, this would compute ⟨n_i⟩ from the quantum state\n",
        "        concentrations = []\n",
        "\n",
        "        for comp_idx in range(4):  # 4 compartments\n",
        "            start_qubit = comp_idx * 3\n",
        "            # Approximate concentration from quantum state probabilities\n",
        "            concentration = 0\n",
        "            for level in range(self.d_levels):\n",
        "                # Binary representation of level\n",
        "                binary = [(level >> i) & 1 for i in range(3)]\n",
        "                # Find probability of this configuration in the relevant qubits\n",
        "                prob_contribution = np.sum(quantum_probs) * level / self.d_levels  # Simplified\n",
        "                concentration += prob_contribution\n",
        "\n",
        "            concentrations.append(max(0, concentration))\n",
        "\n",
        "        return concentrations\n",
        "\n",
        "    def simulate_quantum_pkpd(self, params, times, doses, dose_times, bw, comed, baseline_R=8.0):\n",
        "        \"\"\"\n",
        "        Simulate PK/PD using quantum Lindblad evolution\n",
        "        Fallback to classical if quantum simulation fails\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Get Lindblad rates\n",
        "            rates = self.create_lindblad_operators(params, bw, comed)\n",
        "\n",
        "            # Time stepping\n",
        "            dt = times[1] - times[0] if len(times) > 1 else 0.1\n",
        "            dose_interp = interp1d(dose_times, doses, kind='linear', bounds_error=False, fill_value=0) if len(dose_times) > 0 else lambda t: 0\n",
        "\n",
        "            concentrations_over_time = []\n",
        "            response_over_time = []\n",
        "\n",
        "            # Initial quantum state evolution\n",
        "            for t in times:\n",
        "                dose_rate = dose_interp(t)\n",
        "\n",
        "                # Evolve quantum state for one time step\n",
        "                quantum_probs = self.quantum_evolution_circuit(params, rates, dt, dose_rate)\n",
        "\n",
        "                # Measure concentrations\n",
        "                measured_concs = self.measure_concentrations(quantum_probs)\n",
        "\n",
        "                # Convert to physical units (same as classical)\n",
        "                V1_i = params[1] * ((bw/70)**params[11])\n",
        "                conc_central = measured_concs[0] / V1_i  # A1 concentration\n",
        "                response = measured_concs[3]  # R response\n",
        "\n",
        "                concentrations_over_time.append(max(0, conc_central))\n",
        "                response_over_time.append(max(0, response))\n",
        "\n",
        "            return np.array(concentrations_over_time), np.array(response_over_time)\n",
        "\n",
        "        except Exception as e:\n",
        "            # Fallback to classical simulation\n",
        "            return None, None\n",
        "\n",
        "# JAX-optimized classical fallback (same ODE system as original)\n",
        "@jit\n",
        "def pk_pd_system_jax(t, y, dose_rate, params, bw, comed):\n",
        "    \"\"\"JAX-compiled classical ODE system (same as original numba version)\"\"\"\n",
        "    A1, A2, AE, R = y\n",
        "\n",
        "    CL, V1, Q, V2, KA = params[0], params[1], params[2], params[3], params[4]\n",
        "    KE0, IMAX, IC50, KIN, KOUT = params[5], params[6], params[7], params[8], params[9]\n",
        "    CLBW, V1BW, CLCOMED, KINCOMED = params[10], params[11], params[12], params[13]\n",
        "\n",
        "    CL_i = CL * ((bw/70.0)**CLBW) * (1.0 + CLCOMED * comed)\n",
        "    V1_i = V1 * ((bw/70.0)**V1BW)\n",
        "    KIN_i = KIN * (1.0 + KINCOMED * comed)\n",
        "\n",
        "    C1 = A1 / V1_i\n",
        "    CE = AE / V1_i\n",
        "\n",
        "    dA1_dt = KA * dose_rate - (CL_i/V1_i + Q/V1_i) * A1 + (Q/V2) * A2\n",
        "    dA2_dt = (Q/V1_i) * A1 - (Q/V2) * A2\n",
        "    dAE_dt = KE0 * A1 - KE0 * AE\n",
        "\n",
        "    inhibition = (IMAX * CE) / (IC50 + CE)\n",
        "    dR_dt = KIN_i * (1.0 - inhibition) - KOUT * R\n",
        "\n",
        "    return jnp.array([dA1_dt, dA2_dt, dAE_dt, dR_dt])\n",
        "\n",
        "@numba_jit(nopython=True)\n",
        "def pk_pd_system_numba(t, y, dose_rate, params, bw, comed):\n",
        "    \"\"\"Numba-compiled classical ODE system (same as original)\"\"\"\n",
        "    A1, A2, AE, R = y\n",
        "\n",
        "    CL, V1, Q, V2, KA = params[0], params[1], params[2], params[3], params[4]\n",
        "    KE0, IMAX, IC50, KIN, KOUT = params[5], params[6], params[7], params[8], params[9]\n",
        "    CLBW, V1BW, CLCOMED, KINCOMED = params[10], params[11], params[12], params[13]\n",
        "\n",
        "    CL_i = CL * ((bw/70.0)**CLBW) * (1.0 + CLCOMED * comed)\n",
        "    V1_i = V1 * ((bw/70.0)**V1BW)\n",
        "    KIN_i = KIN * (1.0 + KINCOMED * comed)\n",
        "\n",
        "    C1 = A1 / V1_i\n",
        "    CE = AE / V1_i\n",
        "\n",
        "    dA1_dt = KA * dose_rate - (CL_i/V1_i + Q/V1_i) * A1 + (Q/V2) * A2\n",
        "    dA2_dt = (Q/V1_i) * A1 - (Q/V2) * A2\n",
        "    dAE_dt = KE0 * A1 - KE0 * AE\n",
        "\n",
        "    inhibition = (IMAX * CE) / (IC50 + CE)\n",
        "    dR_dt = KIN_i * (1.0 - inhibition) - KOUT * R\n",
        "\n",
        "    return np.array([dA1_dt, dA2_dt, dAE_dt, dR_dt])\n",
        "\n",
        "class PK_PD_Model:\n",
        "    \"\"\"\n",
        "    Enhanced PK/PD model with quantum backend option (same interface as original)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Same parameter names as original\n",
        "        self.pk_params = ['CL', 'V1', 'Q', 'V2', 'KA']\n",
        "        self.pd_params = ['KE0', 'IMAX', 'IC50', 'KIN', 'KOUT']\n",
        "        self.covariate_params = ['CLBW', 'V1BW', 'CLCOMED', 'KINCOMED']\n",
        "        self.error_params = ['SIGMA_PK', 'SIGMA_PD']\n",
        "\n",
        "        self.all_params = (self.pk_params + self.pd_params +\n",
        "                          self.covariate_params + self.error_params)\n",
        "\n",
        "        # Initialize quantum simulator\n",
        "        self.quantum_simulator = QuantumCompartmentSimulator()\n",
        "\n",
        "        # Cached solutions (same as original)\n",
        "        self.solution_cache = {}\n",
        "        self.cache_tolerance = 1e-2\n",
        "\n",
        "    def pk_pd_system(self, t, y, dose_func, params, bw, comed):\n",
        "        \"\"\"Traditional ODE system with performance monitoring (same as original)\"\"\"\n",
        "        perf_monitor.log_metric('ode_evaluations', 1, increment=True)\n",
        "        dose_rate = dose_func(t)\n",
        "        return pk_pd_system_numba(t, y, dose_rate, params, bw, comed)\n",
        "\n",
        "    def get_cache_key(self, params, bw, comed, doses, dose_times):\n",
        "        \"\"\"Create cache key (same as original)\"\"\"\n",
        "        key_params = tuple(np.round(params, 3))\n",
        "        key_bw = round(bw, 1)\n",
        "        key_comed = int(comed)\n",
        "        key_doses = tuple(np.round(doses, 1))\n",
        "        key_dose_times = tuple(np.round(dose_times, 1))\n",
        "        return (key_params, key_bw, key_comed, key_doses, key_dose_times)\n",
        "\n",
        "    def simulate_individual_fast(self, params, times, doses, dose_times, bw, comed, baseline_R=8.0):\n",
        "        \"\"\"\n",
        "        Enhanced simulation with quantum backend option (same interface as original)\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Check cache first (same as original)\n",
        "        cache_key = self.get_cache_key(params, bw, comed, doses, dose_times)\n",
        "        if cache_key in self.solution_cache:\n",
        "            cached_times, cached_conc, cached_response = self.solution_cache[cache_key]\n",
        "            if np.allclose(times, cached_times, rtol=1e-2):\n",
        "                conc = np.interp(times, cached_times, cached_conc)\n",
        "                response = np.interp(times, cached_times, cached_response)\n",
        "                perf_monitor.log_metric('ode_solve_time', time.time() - start_time, increment=True)\n",
        "                return conc, response\n",
        "\n",
        "        # Try quantum simulation first\n",
        "        if self.quantum_simulator.use_quantum:\n",
        "            conc, response = self.quantum_simulator.simulate_quantum_pkpd(\n",
        "                params, times, doses, dose_times, bw, comed, baseline_R\n",
        "            )\n",
        "\n",
        "            if conc is not None and response is not None:\n",
        "                if len(self.solution_cache) < 10000:\n",
        "                    self.solution_cache[cache_key] = (times.copy(), conc.copy(), response.copy())\n",
        "\n",
        "                perf_monitor.log_metric('ode_solve_time', time.time() - start_time, increment=True)\n",
        "                perf_monitor.log_metric('successful_subjects', 1, increment=True)\n",
        "                return conc, response\n",
        "\n",
        "        # Classical fallback (same as original optimize solver)\n",
        "        dose_rates = np.zeros(len(times))\n",
        "        dt = times[1] - times[0] if len(times) > 1 else 0.1\n",
        "\n",
        "        # Create dose schedule\n",
        "        for i in range(len(dose_times)):\n",
        "            time_idx = int((dose_times[i] - times[0]) / dt)\n",
        "            if 0 <= time_idx < len(dose_rates):\n",
        "                dose_rates[time_idx] = doses[i] / dt\n",
        "\n",
        "        dose_interp = interp1d(times, dose_rates, kind='linear', bounds_error=False, fill_value=0)\n",
        "\n",
        "        def dose_func(t):\n",
        "            return dose_interp(t)\n",
        "\n",
        "        y0 = [0, 0, 0, baseline_R]\n",
        "\n",
        "        try:\n",
        "            # Use optimized solver settings (same as original)\n",
        "            sol = solve_ivp(\n",
        "                lambda t, y: self.pk_pd_system(t, y, dose_func, params, bw, comed),\n",
        "                [times[0], times[-1]], y0, t_eval=times,\n",
        "                method='DOP853',\n",
        "                rtol=1e-4, atol=1e-7,\n",
        "                max_step=1.0\n",
        "            )\n",
        "\n",
        "            if sol.success:\n",
        "                A1, A2, AE, R = sol.y\n",
        "\n",
        "                V1BW = params[11]\n",
        "                V1_i = params[1] * (bw/70)**V1BW\n",
        "                concentrations = A1 / V1_i\n",
        "\n",
        "                # Cache successful solutions (same as original)\n",
        "                if len(self.solution_cache) < 10000:\n",
        "                    self.solution_cache[cache_key] = (times.copy(), concentrations.copy(), R.copy())\n",
        "\n",
        "                perf_monitor.log_metric('ode_solve_time', time.time() - start_time, increment=True)\n",
        "                perf_monitor.log_metric('successful_subjects', 1, increment=True)\n",
        "                return concentrations, R\n",
        "            else:\n",
        "                perf_monitor.log_metric('failed_subjects', 1, increment=True)\n",
        "                return np.full_like(times, np.nan), np.full_like(times, np.nan)\n",
        "\n",
        "        except Exception as e:\n",
        "            perf_monitor.log_metric('failed_subjects', 1, increment=True)\n",
        "            perf_monitor.log_metric('ode_solve_time', time.time() - start_time, increment=True)\n",
        "            return np.full_like(times, np.nan), np.full_like(times, np.nan)\n",
        "\n",
        "    # Keep original method for compatibility\n",
        "    def simulate_individual(self, params, times, doses, dose_times, bw, comed, baseline_R=8.0):\n",
        "        return self.simulate_individual_fast(params, times, doses, dose_times, bw, comed, baseline_R)\n",
        "\n",
        "class QuantumEnhancedSAEM:\n",
        "    \"\"\"\n",
        "    SAEM with quantum-enhanced parameter sampling (same interface as original SAEM_Estimator)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.data = None\n",
        "        self.theta = None\n",
        "        self.omega = None\n",
        "        self.sigma = None\n",
        "\n",
        "        # SAEM parameters (same as original)\n",
        "        self.n_burn_in = 4\n",
        "        self.n_iterations = 20\n",
        "        self.n_chains = 4\n",
        "        self.step_size = 1.0\n",
        "\n",
        "        # Quantum parameter sampler setup\n",
        "        self.n_param_qubits = 6\n",
        "        self.param_dev = qml.device('default.qubit', wires=self.n_param_qubits)\n",
        "\n",
        "    def load_data(self, df):\n",
        "        \"\"\"Load and prepare dataset (same as original)\"\"\"\n",
        "        self.data = df.copy()\n",
        "        self.pk_data = df[(df['DVID'] == 1) & (df['EVID'] == 0) & (df['MDV'] == 0)].copy()\n",
        "        self.pd_data = df[(df['DVID'] == 2) & (df['EVID'] == 0) & (df['MDV'] == 0)].copy()\n",
        "\n",
        "        self.unique_subjects = df['ID'].unique()\n",
        "        self.subject_data_dict = {}\n",
        "\n",
        "        for subject_id in self.unique_subjects:\n",
        "            self.subject_data_dict[subject_id] = df[df['ID'] == subject_id].copy()\n",
        "\n",
        "        print(f\"Loaded {len(self.pk_data)} PK observations and {len(self.pd_data)} PD observations\")\n",
        "        print(f\"Preprocessed {len(self.unique_subjects)} subjects\")\n",
        "\n",
        "    def quantum_parameter_sampler(self, phi_params):\n",
        "        \"\"\"\n",
        "        Quantum variational circuit for parameter sampling\n",
        "        \"\"\"\n",
        "        @qml.qnode(self.param_dev)\n",
        "        def circuit(phi):\n",
        "            # Create ansatz for parameter distribution\n",
        "            for i in range(self.n_param_qubits):\n",
        "                qml.RY(phi[i], wires=i)\n",
        "\n",
        "            # Entangling layer\n",
        "            for i in range(self.n_param_qubits - 1):\n",
        "                qml.CNOT(wires=[i, i + 1])\n",
        "\n",
        "            # Second rotation layer\n",
        "            for i in range(self.n_param_qubits):\n",
        "                qml.RY(phi[i + self.n_param_qubits], wires=i)\n",
        "\n",
        "            return qml.probs(wires=range(self.n_param_qubits))\n",
        "\n",
        "        return circuit(phi_params)\n",
        "\n",
        "    def individual_likelihood_fast(self, eta_i, theta, omega, sigma, subject_data):\n",
        "        \"\"\"Fast likelihood calculation (same as original)\"\"\"\n",
        "        params = theta.copy()\n",
        "\n",
        "        pk_pd_indices = list(range(10))\n",
        "        for i, idx in enumerate(pk_pd_indices):\n",
        "            if i < len(eta_i):\n",
        "                params[idx] *= np.exp(eta_i[i])\n",
        "\n",
        "        bw = subject_data['BW'].iloc[0]\n",
        "        comed = subject_data['COMED'].iloc[0]\n",
        "\n",
        "        pk_obs = subject_data[subject_data['DVID'] == 1]\n",
        "        pd_obs = subject_data[subject_data['DVID'] == 2]\n",
        "\n",
        "        if len(pk_obs) == 0 and len(pd_obs) == 0:\n",
        "            return -np.inf\n",
        "\n",
        "        doses = subject_data[subject_data['EVID'] == 1]['AMT'].values\n",
        "        dose_times = subject_data[subject_data['EVID'] == 1]['TIME'].values\n",
        "\n",
        "        all_times = np.sort(subject_data['TIME'].unique())\n",
        "\n",
        "        try:\n",
        "            # Use enhanced simulation\n",
        "            conc_pred, response_pred = self.model.simulate_individual_fast(\n",
        "                params, all_times, doses, dose_times, bw, comed\n",
        "            )\n",
        "\n",
        "            log_likelihood = 0\n",
        "\n",
        "            # Same likelihood calculations as original\n",
        "            if len(pk_obs) > 0:\n",
        "                pk_times = pk_obs['TIME'].values\n",
        "                pk_observed = pk_obs['DV'].values\n",
        "\n",
        "                pk_predicted = np.interp(pk_times, all_times, conc_pred)\n",
        "                valid_idx = (pk_predicted > 0) & (pk_observed > 0) & np.isfinite(pk_predicted)\n",
        "\n",
        "                if np.sum(valid_idx) > 0:\n",
        "                    pk_residuals = np.log(pk_observed[valid_idx]) - np.log(pk_predicted[valid_idx])\n",
        "                    log_likelihood += -0.5 * np.sum(pk_residuals**2 / sigma[0]**2)\n",
        "                    log_likelihood += -0.5 * len(pk_residuals) * np.log(2 * np.pi * sigma[0]**2)\n",
        "                    log_likelihood += -np.sum(np.log(pk_observed[valid_idx]))\n",
        "\n",
        "            if len(pd_obs) > 0:\n",
        "                pd_times = pd_obs['TIME'].values\n",
        "                pd_observed = pd_obs['DV'].values\n",
        "\n",
        "                pd_predicted = np.interp(pd_times, all_times, response_pred)\n",
        "                valid_idx = (pd_predicted > 0) & (pd_observed > 0) & np.isfinite(pd_predicted)\n",
        "\n",
        "                if np.sum(valid_idx) > 0:\n",
        "                    pd_residuals = pd_observed[valid_idx] - pd_predicted[valid_idx]\n",
        "                    pd_variance = (sigma[1] * pd_predicted[valid_idx])**2\n",
        "                    log_likelihood += -0.5 * np.sum(pd_residuals**2 / pd_variance)\n",
        "                    log_likelihood += -0.5 * np.sum(np.log(2 * np.pi * pd_variance))\n",
        "\n",
        "            # Prior for random effects (same as original)\n",
        "            if len(eta_i) > 0:\n",
        "                log_likelihood += -0.5 * eta_i.T @ np.linalg.solve(omega, eta_i)\n",
        "\n",
        "        except Exception as e:\n",
        "            return -np.inf\n",
        "\n",
        "        return log_likelihood\n",
        "\n",
        "    def quantum_mcmc_step(self, eta, theta, omega, sigma, subject_data, step_size):\n",
        "        \"\"\"\n",
        "        Enhanced MCMC step with quantum parameter proposals\n",
        "        \"\"\"\n",
        "        # Generate quantum-enhanced proposal (simplified implementation)\n",
        "        try:\n",
        "            phi_params = np.random.uniform(0, 2*np.pi, 2 * self.n_param_qubits)\n",
        "\n",
        "            # Set device for this qnode\n",
        "            quantum_sampler = qml.QNode(self.quantum_parameter_sampler, self.param_dev)\n",
        "            probs = quantum_sampler(phi_params)\n",
        "\n",
        "            # Generate proposal from quantum distribution\n",
        "            sample_idx = np.random.choice(len(probs), p=probs)\n",
        "            eta_prop = eta + np.random.normal(0, step_size, len(eta))\n",
        "\n",
        "        except:\n",
        "            # Fallback to classical proposal\n",
        "            eta_prop = eta + np.random.normal(0, step_size, len(eta))\n",
        "\n",
        "        # Classical Metropolis acceptance (same as original)\n",
        "        ll_current = self.individual_likelihood_fast(eta, theta, omega, sigma, subject_data)\n",
        "        ll_prop = self.individual_likelihood_fast(eta_prop, theta, omega, sigma, subject_data)\n",
        "\n",
        "        alpha = min(1, np.exp(ll_prop - ll_current))\n",
        "\n",
        "        if np.random.random() < alpha:\n",
        "            return eta_prop, ll_prop\n",
        "        else:\n",
        "            return eta, ll_current\n",
        "\n",
        "    def mcmc_step(self, eta, theta, omega, sigma, subject_data, step_size):\n",
        "        \"\"\"Wrapper to maintain original interface\"\"\"\n",
        "        return self.quantum_mcmc_step(eta, theta, omega, sigma, subject_data, step_size)\n",
        "\n",
        "    def saem_iteration(self, theta, omega, sigma):\n",
        "        \"\"\"SAEM iteration (same structure as original)\"\"\"\n",
        "        def process_subject(subject_id):\n",
        "            subject_data = self.subject_data_dict[subject_id]\n",
        "            eta = np.random.multivariate_normal(np.zeros(len(omega)), omega * 0.1)\n",
        "\n",
        "            # MCMC steps for this subject\n",
        "            for _ in range(3):\n",
        "                eta, _ = self.quantum_mcmc_step(eta, theta, omega, sigma, subject_data, self.step_size)\n",
        "\n",
        "            return subject_id, eta\n",
        "\n",
        "        # Parallel processing (same as original)\n",
        "        with ThreadPoolExecutor(max_workers=min(mp.cpu_count(), len(self.unique_subjects))) as executor:\n",
        "            results = list(executor.map(process_subject, self.unique_subjects))\n",
        "\n",
        "        eta_estimates = {}\n",
        "        for subject_id, eta in results:\n",
        "            eta_estimates[subject_id] = eta\n",
        "\n",
        "        return eta_estimates\n",
        "\n",
        "    def update_parameters(self, eta_estimates, iteration):\n",
        "        \"\"\"Update parameters (same as original)\"\"\"\n",
        "        gamma = min(1.0, 10.0 / (iteration + 10))\n",
        "        eta_values = np.array(list(eta_estimates.values()))\n",
        "\n",
        "        if len(eta_values) > 0:\n",
        "            empirical_cov = np.cov(eta_values.T)\n",
        "            self.omega = (1 - gamma) * self.omega + gamma * empirical_cov\n",
        "\n",
        "            # Ensure positive definiteness\n",
        "            eigenvals, eigenvecs = np.linalg.eigh(self.omega)\n",
        "            eigenvals = np.maximum(eigenvals, 1e-6)\n",
        "            self.omega = eigenvecs @ np.diag(eigenvals) @ eigenvecs.T\n",
        "\n",
        "    def fit(self, initial_params=None):\n",
        "        \"\"\"Fit using enhanced SAEM (same interface as original)\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        if initial_params is None:\n",
        "            initial_params = np.array([\n",
        "                2.0, 10.0, 1.0, 20.0, 0.5,  # PK\n",
        "                0.1, 0.8, 2.0, 5.0, 0.1,    # PD\n",
        "                0.75, 1.0, 0.1, 0.1          # Covariates\n",
        "            ])\n",
        "\n",
        "        self.theta = initial_params.copy()\n",
        "        self.omega = np.eye(10) * 0.1\n",
        "        self.sigma = np.array([0.2, 0.15])\n",
        "\n",
        "        print(\"Starting SAEM parameter estimation...\")\n",
        "        print(f\"Burn-in: {self.n_burn_in} iterations\")\n",
        "        print(f\"Estimation: {self.n_iterations - self.n_burn_in} iterations\")\n",
        "\n",
        "        best_ll = -np.inf\n",
        "        best_params = self.theta.copy()\n",
        "\n",
        "        # SAEM iterations (same as original)\n",
        "        for iteration in tqdm(range(self.n_iterations), desc=\"SAEM Progress\"):\n",
        "            eta_estimates = self.saem_iteration(self.theta, self.omega, self.sigma)\n",
        "\n",
        "            if iteration > self.n_burn_in:\n",
        "                self.update_parameters(eta_estimates, iteration - self.n_burn_in)\n",
        "\n",
        "            # Monitor convergence (same as original)\n",
        "            if iteration % 50 == 0:\n",
        "                total_ll = 0\n",
        "                n_successful = 0\n",
        "\n",
        "                for subject_id in list(self.unique_subjects)[:min(50, len(self.unique_subjects))]:\n",
        "                    try:\n",
        "                        eta = eta_estimates.get(subject_id, np.zeros(len(self.omega)))\n",
        "                        ll = self.individual_likelihood_fast(\n",
        "                            eta, self.theta, self.omega, self.sigma,\n",
        "                            self.subject_data_dict[subject_id]\n",
        "                        )\n",
        "                        if np.isfinite(ll):\n",
        "                            total_ll += ll\n",
        "                            n_successful += 1\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "                if n_successful > 0:\n",
        "                    avg_ll = total_ll / n_successful\n",
        "                    if avg_ll > best_ll:\n",
        "                        best_ll = avg_ll\n",
        "                        best_params = self.theta.copy()\n",
        "\n",
        "                    print(f\"Iteration {iteration}: Avg LL = {avg_ll:.2f}, Best = {best_ll:.2f}\")\n",
        "\n",
        "            perf_monitor.log_metric('convergence_iterations', 1, increment=True)\n",
        "\n",
        "        self.theta = best_params\n",
        "        perf_monitor.log_metric('optimization_time', time.time() - start_time, increment=True)\n",
        "\n",
        "        print(f\"\\nSAEM converged after {self.n_iterations} iterations\")\n",
        "        print(\"Final parameter estimates:\")\n",
        "        for i, param_name in enumerate(self.model.all_params[:-2]):\n",
        "            if i < len(self.theta):\n",
        "                print(f\"  {param_name}: {self.theta[i]:.4f}\")\n",
        "\n",
        "        return self.theta, best_ll\n",
        "\n",
        "# Keep original class name for compatibility\n",
        "SAEM_Estimator = QuantumEnhancedSAEM\n",
        "NLME_Estimator = QuantumEnhancedSAEM\n",
        "\n",
        "class PopulationSimulator:\n",
        "    \"\"\"\n",
        "    Enhanced population simulator (same interface as original)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, theta, omega, sigma):\n",
        "        self.model = model\n",
        "        self.theta = theta\n",
        "        self.omega = omega\n",
        "        self.sigma = sigma\n",
        "        self.batch_size = 60\n",
        "\n",
        "    def generate_virtual_population_fast(self, n_subjects, bw_range=(50, 100), comed_prob=0.5, seed=None):\n",
        "        \"\"\"Generate virtual population (same as original)\"\"\"\n",
        "        if seed:\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        body_weights = np.random.uniform(bw_range[0], bw_range[1], n_subjects)\n",
        "        comed_status = np.random.binomial(1, comed_prob, n_subjects)\n",
        "\n",
        "        try:\n",
        "            eta_samples = np.random.multivariate_normal(\n",
        "                np.zeros(len(self.omega)), self.omega, size=n_subjects\n",
        "            )\n",
        "        except np.linalg.LinAlgError:\n",
        "            eta_samples = np.random.normal(0, 0.1, (n_subjects, len(self.omega)))\n",
        "\n",
        "        virtual_population = []\n",
        "        theta_broadcast = np.broadcast_to(self.theta, (n_subjects, len(self.theta)))\n",
        "\n",
        "        for i in range(n_subjects):\n",
        "            individual_params = theta_broadcast[i].copy()\n",
        "\n",
        "            for j in range(min(eta_samples.shape[1], 10)):\n",
        "                if j < len(individual_params):\n",
        "                    individual_params[j] *= np.exp(eta_samples[i, j])\n",
        "\n",
        "            virtual_population.append({\n",
        "                'subject_id': i,\n",
        "                'bw': body_weights[i],\n",
        "                'comed': comed_status[i],\n",
        "                'params': individual_params,\n",
        "                'eta': eta_samples[i]\n",
        "            })\n",
        "\n",
        "        return virtual_population\n",
        "\n",
        "    def simulate_batch(self, batch_subjects, dose_mg, dosing_interval_h, simulation_days=28, steady_state_days=21):\n",
        "        \"\"\"Simulate batch (same as original)\"\"\"\n",
        "        def simulate_single(subject):\n",
        "            dt = 1.0\n",
        "            total_hours = simulation_days * 24\n",
        "            times = np.arange(0, total_hours + dt, dt)\n",
        "\n",
        "            dose_times = np.arange(0, total_hours, dosing_interval_h)\n",
        "            doses = np.full(len(dose_times), dose_mg)\n",
        "\n",
        "            try:\n",
        "                conc, response = self.model.simulate_individual_fast(\n",
        "                    subject['params'], times, doses, dose_times,\n",
        "                    subject['bw'], subject['comed']\n",
        "                )\n",
        "\n",
        "                if not (np.isnan(conc).all() or np.isnan(response).all()):\n",
        "                    steady_start_idx = int(steady_state_days * 24 / dt)\n",
        "                    steady_response = response[steady_start_idx:]\n",
        "                    steady_conc = conc[steady_start_idx:]\n",
        "\n",
        "                    if dosing_interval_h == 24:\n",
        "                        interval_size = int(24/dt)\n",
        "                    else:\n",
        "                        interval_size = int(168/dt)\n",
        "\n",
        "                    target_achieved = True\n",
        "                    for start in range(0, len(steady_response), interval_size):\n",
        "                        end = min(start + interval_size, len(steady_response))\n",
        "                        interval_response = steady_response[start:end]\n",
        "                        if len(interval_response) > 0 and not np.all(interval_response < 3.3):\n",
        "                            target_achieved = False\n",
        "                            break\n",
        "\n",
        "                    return {\n",
        "                        'subject_id': subject['subject_id'],\n",
        "                        'bw': subject['bw'],\n",
        "                        'comed': subject['comed'],\n",
        "                        'target_achieved': target_achieved,\n",
        "                        'min_response': np.min(steady_response),\n",
        "                        'mean_response': np.mean(steady_response),\n",
        "                        'max_conc': np.max(steady_conc)\n",
        "                    }\n",
        "                else:\n",
        "                    return None\n",
        "            except:\n",
        "                return None\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=min(len(batch_subjects), mp.cpu_count())) as executor:\n",
        "            results = list(executor.map(simulate_single, batch_subjects))\n",
        "\n",
        "        return [r for r in results if r is not None]\n",
        "\n",
        "    def simulate_dosing_regimen(self, virtual_population, dose_mg, dosing_interval_h, simulation_days=28, steady_state_days=21):\n",
        "        \"\"\"Simulate dosing regimen (same as original)\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        print(f\"Simulating {dose_mg} mg every {dosing_interval_h}h for {len(virtual_population)} subjects...\")\n",
        "        print(f\"Using batch processing with {self.batch_size} subjects per batch\")\n",
        "\n",
        "        all_results = []\n",
        "\n",
        "        for i in tqdm(range(0, len(virtual_population), self.batch_size), desc=\"Batch Progress\"):\n",
        "            batch_end = min(i + self.batch_size, len(virtual_population))\n",
        "            batch_subjects = virtual_population[i:batch_end]\n",
        "\n",
        "            batch_results = self.simulate_batch(\n",
        "                batch_subjects, dose_mg, dosing_interval_h,\n",
        "                simulation_days, steady_state_days\n",
        "            )\n",
        "            all_results.extend(batch_results)\n",
        "\n",
        "        perf_monitor.log_metric('simulation_time', time.time() - start_time, increment=True)\n",
        "        perf_monitor.log_metric('successful_subjects', len(all_results), increment=True)\n",
        "        perf_monitor.log_metric('failed_subjects', len(virtual_population) - len(all_results), increment=True)\n",
        "\n",
        "        print(f\"Successfully simulated {len(all_results)}/{len(virtual_population)} subjects\")\n",
        "\n",
        "        return pd.DataFrame(all_results)\n",
        "\n",
        "    def find_optimal_dose_adaptive(self, target_achievement=0.9, dose_range=(0.5, 20), dosing_interval=24, n_subjects=5000, **population_kwargs):\n",
        "        \"\"\"Adaptive dose optimization (same as original)\"\"\"\n",
        "        print(f\"\\nAdaptive dose optimization for {target_achievement*100}% target achievement\")\n",
        "        print(f\"Dosing interval: {dosing_interval} hours\")\n",
        "\n",
        "        virtual_pop = self.generate_virtual_population_fast(n_subjects, **population_kwargs)\n",
        "\n",
        "        kernel = Matern(length_scale=1.0, nu=2.5) + WhiteKernel(noise_level=0.01)\n",
        "        gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=2)\n",
        "\n",
        "        initial_doses = np.linspace(dose_range[0], dose_range[1], 5)\n",
        "        dose_history = []\n",
        "        achievement_history = []\n",
        "\n",
        "        for dose in initial_doses:\n",
        "            if dosing_interval == 24:\n",
        "                test_dose = dose\n",
        "            else:\n",
        "                test_dose = dose * 7\n",
        "\n",
        "            results = self.simulate_dosing_regimen(virtual_pop, test_dose, dosing_interval)\n",
        "\n",
        "            if len(results) > 0:\n",
        "                achievement = results['target_achieved'].mean()\n",
        "                dose_history.append(dose)\n",
        "                achievement_history.append(achievement)\n",
        "                print(f\"  Dose {dose:.1f} mg: {achievement:.1%} achievement\")\n",
        "\n",
        "        if len(dose_history) >= 3:\n",
        "            X = np.array(dose_history).reshape(-1, 1)\n",
        "            y = np.array(achievement_history)\n",
        "            gp.fit(X, y)\n",
        "\n",
        "        # Adaptive optimization (same as original)\n",
        "        for iteration in range(10):\n",
        "            if len(dose_history) < 3:\n",
        "                break\n",
        "\n",
        "            test_doses = np.linspace(dose_range[0], dose_range[1], 100)\n",
        "            mean_pred, std_pred = gp.predict(test_doses.reshape(-1, 1), return_std=True)\n",
        "\n",
        "            beta = 2.0\n",
        "            acquisition = mean_pred + beta * std_pred\n",
        "\n",
        "            target_diff = np.abs(mean_pred - target_achievement)\n",
        "            acquisition_adjusted = acquisition - target_diff\n",
        "\n",
        "            next_dose_idx = np.argmax(acquisition_adjusted)\n",
        "            next_dose = test_doses[next_dose_idx]\n",
        "\n",
        "            if min(np.abs(np.array(dose_history) - next_dose)) < 0.2:\n",
        "                break\n",
        "\n",
        "            if dosing_interval == 24:\n",
        "                test_dose = next_dose\n",
        "            else:\n",
        "                test_dose = next_dose * 7\n",
        "\n",
        "            results = self.simulate_dosing_regimen(virtual_pop, test_dose, dosing_interval)\n",
        "\n",
        "            if len(results) > 0:\n",
        "                achievement = results['target_achieved'].mean()\n",
        "                dose_history.append(next_dose)\n",
        "                achievement_history.append(achievement)\n",
        "\n",
        "                print(f\"  Adaptive iteration {iteration+1}: Dose {next_dose:.1f} mg: {achievement:.1%}\")\n",
        "\n",
        "                X = np.array(dose_history).reshape(-1, 1)\n",
        "                y = np.array(achievement_history)\n",
        "                gp.fit(X, y)\n",
        "\n",
        "                if abs(achievement - target_achievement) < 0.02:\n",
        "                    print(f\"  Converged at dose {next_dose:.1f} mg\")\n",
        "                    return round(next_dose * 2) / 2 if dosing_interval == 24 else round(next_dose / 5) * 5\n",
        "\n",
        "        achievement_array = np.array(achievement_history)\n",
        "        target_mask = achievement_array >= target_achievement\n",
        "\n",
        "        if np.any(target_mask):\n",
        "            valid_doses = np.array(dose_history)[target_mask]\n",
        "            optimal_dose = np.min(valid_doses)\n",
        "        else:\n",
        "            optimal_dose = dose_history[np.argmax(achievement_array)]\n",
        "\n",
        "        if dosing_interval == 24:\n",
        "            optimal_dose = round(optimal_dose * 2) / 2\n",
        "        else:\n",
        "            optimal_dose = round(optimal_dose / 5) * 5\n",
        "\n",
        "        return optimal_dose\n",
        "\n",
        "    # Keep original method names for compatibility\n",
        "    def find_optimal_dose(self, target_achievement=0.9, dose_range=(0.5, 20), dosing_interval=24, n_subjects=5000, **population_kwargs):\n",
        "        return self.find_optimal_dose_adaptive(target_achievement, dose_range, dosing_interval, n_subjects, **population_kwargs)\n",
        "\n",
        "    def generate_virtual_population(self, n_subjects, **kwargs):\n",
        "        return self.generate_virtual_population_fast(n_subjects, **kwargs)\n",
        "\n",
        "def run_complete_simulation_enhanced(df):\n",
        "    \"\"\"\n",
        "    Complete simulation with quantum enhancement (same interface and outputs as original)\n",
        "    \"\"\"\n",
        "    perf_monitor.reset()\n",
        "\n",
        "    print(\"STARTING ENHANCED PKPD SIMULATION\")\n",
        "    print(f\"CPU Cores Available: {mp.cpu_count()}\")\n",
        "    print(\"Quantum Enhancement: PennyLane-based compartment simulation\")\n",
        "\n",
        "    # Initialize enhanced model and estimator (same interface)\n",
        "    model = PK_PD_Model()\n",
        "    estimator = SAEM_Estimator(model)\n",
        "    estimator.load_data(df)\n",
        "\n",
        "    # Fit model (same interface)\n",
        "    print(\"\\nFITTING NLME MODEL WITH SAEM...\")\n",
        "    final_params, final_ll = estimator.fit()\n",
        "\n",
        "    # Initialize simulator (same interface)\n",
        "    simulator = PopulationSimulator(model, estimator.theta, estimator.omega, estimator.sigma)\n",
        "\n",
        "    # Dose optimization\n",
        "    print(\"ENHANCED DOSE OPTIMIZATION RESULTS\")\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    print(\"\\n1. Daily dose for 90% target achievement (original population):\")\n",
        "    daily_dose_90 = simulator.find_optimal_dose_adaptive(\n",
        "        target_achievement=0.9, dosing_interval=24,\n",
        "        bw_range=(50, 100), comed_prob=0.5, n_subjects=200\n",
        "    )\n",
        "    results['daily_90_original'] = daily_dose_90\n",
        "    print(f\"   Optimal daily dose: {daily_dose_90:.1f} mg\")\n",
        "\n",
        "    print(\"\\n2. Weekly dose for 90% target achievement (original population):\")\n",
        "    weekly_dose_90 = simulator.find_optimal_dose_adaptive(\n",
        "        target_achievement=0.9, dosing_interval=168,\n",
        "        bw_range=(50, 100), comed_prob=0.5, n_subjects=200\n",
        "    )\n",
        "    results['weekly_90_original'] = weekly_dose_90\n",
        "    print(f\"   Optimal weekly dose: {weekly_dose_90:.0f} mg\")\n",
        "\n",
        "    print(\"\\n3. Effect of changed body weight distribution (70-140 kg):\")\n",
        "    daily_dose_90_heavy = simulator.find_optimal_dose_adaptive(\n",
        "        target_achievement=0.9, dosing_interval=24,\n",
        "        bw_range=(70, 140), comed_prob=0.5, n_subjects=200\n",
        "    )\n",
        "    weekly_dose_90_heavy = simulator.find_optimal_dose_adaptive(\n",
        "        target_achievement=0.9, dosing_interval=168,\n",
        "        bw_range=(70, 140), comed_prob=0.5, n_subjects=200\n",
        "    )\n",
        "    results['daily_90_heavy'] = daily_dose_90_heavy\n",
        "    results['weekly_90_heavy'] = weekly_dose_90_heavy\n",
        "    print(f\"   Daily dose (heavy population): {daily_dose_90_heavy:.1f} mg\")\n",
        "    print(f\"   Weekly dose (heavy population): {weekly_dose_90_heavy:.0f} mg\")\n",
        "\n",
        "    print(\"\\n4. Effect of restricting concomitant medication:\")\n",
        "    daily_dose_90_no_comed = simulator.find_optimal_dose_adaptive(\n",
        "        target_achievement=0.9, dosing_interval=24,\n",
        "        bw_range=(50, 100), comed_prob=0.0, n_subjects=200\n",
        "    )\n",
        "    weekly_dose_90_no_comed = simulator.find_optimal_dose_adaptive(\n",
        "        target_achievement=0.9, dosing_interval=168,\n",
        "        bw_range=(50, 100), comed_prob=0.0, n_subjects=200\n",
        "    )\n",
        "    results['daily_90_no_comed'] = daily_dose_90_no_comed\n",
        "    results['weekly_90_no_comed'] = weekly_dose_90_no_comed\n",
        "    print(f\"   Daily dose (no COMED): {daily_dose_90_no_comed:.1f} mg\")\n",
        "    print(f\"   Weekly dose (no COMED): {weekly_dose_90_no_comed:.0f} mg\")\n",
        "\n",
        "    print(\"\\n5. Doses for 75% target achievement:\")\n",
        "\n",
        "    scenarios_75 = [\n",
        "        ('original', (50, 100), 0.5),\n",
        "        ('heavy', (70, 140), 0.5),\n",
        "        ('no_comed', (50, 100), 0.0)\n",
        "    ]\n",
        "\n",
        "    for scenario_name, bw_range, comed_prob in scenarios_75:\n",
        "        daily_dose_75 = simulator.find_optimal_dose_adaptive(\n",
        "            target_achievement=0.75, dosing_interval=24,\n",
        "            bw_range=bw_range, comed_prob=comed_prob, n_subjects=200\n",
        "        )\n",
        "        weekly_dose_75 = simulator.find_optimal_dose_adaptive(\n",
        "            target_achievement=0.75, dosing_interval=168,\n",
        "            bw_range=bw_range, comed_prob=comed_prob, n_subjects=200\n",
        "        )\n",
        "\n",
        "        results[f'daily_75_{scenario_name}'] = daily_dose_75\n",
        "        results[f'weekly_75_{scenario_name}'] = weekly_dose_75\n",
        "\n",
        "        print(f\"   {scenario_name.replace('_', ' ').title()} population:\")\n",
        "        print(f\"     Daily (75%): {daily_dose_75:.1f} mg\")\n",
        "        print(f\"     Weekly (75%): {weekly_dose_75:.0f} mg\")\n",
        "\n",
        "    # Enhanced summary (same format as original)\n",
        "    print(\"ENHANCED SUMMARY OF OPTIMAL DOSES\")\n",
        "\n",
        "    summary_data = []\n",
        "    scenarios = ['original', 'heavy', 'no_comed']\n",
        "    scenario_names = ['Original Pop', 'Heavy Pop (70-140kg)', 'No COMED']\n",
        "\n",
        "    for scenario, name in zip(scenarios, scenario_names):\n",
        "        summary_data.extend([\n",
        "            {\n",
        "                'Scenario': f'{name} (90%)',\n",
        "                'Daily (mg)': results[f'daily_90_{scenario}'],\n",
        "                'Weekly (mg)': results[f'weekly_90_{scenario}'],\n",
        "                'Daily vs Weekly Ratio': results[f'weekly_90_{scenario}'] / (results[f'daily_90_{scenario}'] * 7)\n",
        "            },\n",
        "            {\n",
        "                'Scenario': f'{name} (75%)',\n",
        "                'Daily (mg)': results[f'daily_75_{scenario}'],\n",
        "                'Weekly (mg)': results[f'weekly_75_{scenario}'],\n",
        "                'Daily vs Weekly Ratio': results[f'weekly_75_{scenario}'] / (results[f'daily_75_{scenario}'] * 7)\n",
        "            }\n",
        "        ])\n",
        "\n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    print(summary_df.to_string(index=False, float_format='%.2f'))\n",
        "\n",
        "    # Dose reduction analysis (same as original)\n",
        "    print(\"\\nENHANCED DOSE REDUCTION ANALYSIS (75% vs 90% achievement):\")\n",
        "    for scenario, name in zip(scenarios, scenario_names):\n",
        "        daily_90_key = f'daily_90_{scenario}'\n",
        "        daily_75_key = f'daily_75_{scenario}'\n",
        "        weekly_90_key = f'weekly_90_{scenario}'\n",
        "        weekly_75_key = f'weekly_75_{scenario}'\n",
        "\n",
        "        daily_reduction = results[daily_90_key] - results[daily_75_key]\n",
        "        weekly_reduction = results[weekly_90_key] - results[weekly_75_key]\n",
        "\n",
        "        daily_pct_reduction = (daily_reduction / results[daily_90_key]) * 100\n",
        "        weekly_pct_reduction = (weekly_reduction / results[weekly_90_key]) * 100\n",
        "\n",
        "        print(f\"   {name}:\")\n",
        "        print(f\"     Daily: -{daily_reduction:.1f} mg ({daily_pct_reduction:.1f}% reduction)\")\n",
        "        print(f\"     Weekly: -{weekly_reduction:.0f} mg ({weekly_pct_reduction:.1f}% reduction)\")\n",
        "        print(f\"     Efficiency gain: {weekly_pct_reduction - daily_pct_reduction:.1f}% better with weekly dosing\")\n",
        "\n",
        "    # Performance report\n",
        "    perf_monitor.report()\n",
        "\n",
        "    return results, estimator, simulator\n",
        "\n",
        "results, estimator, simulator = run_complete_simulation_enhanced(df)\n",
        "create_enhanced_diagnostic_plots(estimator, simulator, results)"
      ],
      "metadata": {
        "id": "Kr464YU8WDpa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}